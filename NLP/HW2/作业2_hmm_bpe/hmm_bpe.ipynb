{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7a16ed",
   "metadata": {},
   "source": [
    "## 任务一：HMM模型用于中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d9a94",
   "metadata": {},
   "source": [
    "任务一评分标准：\n",
    "1. 共有8处TODO需要填写，每个TODO计1-2分，共9分，预计代码量30行；\n",
    "2. 允许自行修改、编写代码完成，对于该情况，请补充注释以便于评分，否则结果不正确将导致较多的扣分；\n",
    "3. 实验报告(python)/用于说明实验的文字块(jupyter notebook)不额外计分，但不写会导致扣分。\n",
    "\n",
    "注：本任务仅在短句子上进行效果测试，因此对概率的计算可直接进行连乘。在实践中，常先对概率取对数，将连乘变为加法来计算，以避免出现数值溢出的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77db9",
   "metadata": {},
   "source": [
    "导入HMM参数，初始化所需的起始概率矩阵，转移概率矩阵，发射概率矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36e0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d25beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hmm_parameters.pkl\", \"rb\") as f:\n",
    "    hmm_parameters = pickle.load(f)\n",
    "\n",
    "# 非断字（B）为第0行，断字（I）为第1行\n",
    "# 发射概率矩阵中，词典大小为65536，以汉字的ord作为行key\n",
    "start_probability = hmm_parameters[\"start_prob\"]  # shape(2,)\n",
    "trans_matrix = hmm_parameters[\"trans_mat\"]  # shape(2, 2)\n",
    "emission_matrix = hmm_parameters[\"emission_mat\"]  # shape(2, 65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3c645f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56262868 0.43737132]\n",
      "[[0.18967134 0.81032866]\n",
      " [0.4821748  0.5178252 ]]\n",
      "9.341673039710104e-06\n"
     ]
    }
   ],
   "source": [
    "print(start_probability)\n",
    "print(trans_matrix)\n",
    "print(emission_matrix[1][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7070152",
   "metadata": {},
   "source": [
    "定义待处理的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87219e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 将input_sentence中的xxx替换为你的姓名（1分）\n",
    "input_sentence = \"马逸川是一名优秀的学生\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035cbc7",
   "metadata": {},
   "source": [
    "实现viterbi算法，并以此进行中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1adac849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sent_orig, start_prob, trans_mat, emission_mat):\n",
    "    \"\"\"\n",
    "    viterbi算法进行中文分词\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        str - 中文分词的结果\n",
    "    \"\"\"\n",
    "    \n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "    \n",
    "    # `dp`用来储存不同位置每种标注（B/I）的最大概率值\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "    \n",
    "    # `path`用来储存最大概率对应的上步B/I选择\n",
    "    #  例如 path[1][7] == 1 意味着第8个（从1开始计数）字符标注I对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[0][5] == 1 意味着第6个字符标注B对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[1][1] == 0 意味着第2个字符标注I对应的最大概率，其前一步的隐状态为0（B）\n",
    "    \n",
    "    path = np.zeros((2, len(sent_ord)), dtype=int)\n",
    "    \n",
    "    #  TODO: 第一个位置的最大概率值计算（1分）\n",
    "    dp[0][0], dp[1][0] = start_prob  \n",
    "    dp[0][0] = dp[0][0] * emission_mat[0][sent_ord[0]]\n",
    "    dp[1][0] = dp[1][0] * emission_mat[1][sent_ord[0]]\n",
    "    \n",
    "    # position 1 : 0 \n",
    "    if (trans_mat[0][0] * dp[0][0] * emission_mat[0][sent_ord[0]] \n",
    "        > trans_mat[1][0] * dp[1][0] * emission_mat[0][sent_ord[0]]) :\n",
    "        \n",
    "        dp[0][1] = trans_mat[0][0] * dp[0][0] * emission_mat[0][sent_ord[0]]\n",
    "        path[0][1] = 0  # 值表示前一位的状态(path is used for labeling)\n",
    "        \n",
    "    else :\n",
    "        dp[0][1] = trans_mat[1][0] * dp[1][0] * emission_mat[0][sent_ord[0]]\n",
    "        path[0][1] = 1\n",
    "    \n",
    "    # position 1 : 1\n",
    "    if (trans_mat[0][1] * dp[0][0] * emission_mat[1][sent_ord[0]]\n",
    "        > trans_mat[1][1] * dp[1][0] * emission_mat[1][sent_ord[0]]) :\n",
    "        dp[1][1] = trans_mat[0][1] * dp[0][0] * emission_mat[1][sent_ord[0]]\n",
    "        path[1][1] = 0\n",
    "    else :\n",
    "        dp[1][1] = trans_mat[1][1] * dp[1][0] * emission_mat[1][sent_ord[0]]\n",
    "        path[1][1] = 1\n",
    "\n",
    "    #  TODO: 其余位置的最大概率值计算（填充dp和path矩阵）（2分）\n",
    "    \n",
    "    length = len(sent_ord)\n",
    "    for i in range(2, length):\n",
    "        \n",
    "        # position i : 0 \n",
    "        if (dp[0][i-1] * trans_mat[0][0] * emission_mat[0][sent_ord[i]]\n",
    "            > dp[1][i-1] * trans_mat[1][0] * emission_mat[0][sent_ord[i]]):\n",
    "            \n",
    "            dp[0][i] = dp[0][i-1] * trans_mat[0][0] * emission_mat[0][sent_ord[i]]\n",
    "            path[0][i] = 0\n",
    "            \n",
    "        else :\n",
    "            dp[0][i] = dp[1][i-1] * trans_mat[1][0] * emission_mat[0][sent_ord[i]]\n",
    "            path[0][i] = 1\n",
    "            \n",
    "        # position i : 1    \n",
    "        if (dp[0][i-1] * trans_mat[0][1] * emission_mat[1][sent_ord[i]]\n",
    "            > dp[1][i-1] * trans_mat[1][1] * emission_mat[1][sent_ord[i]]) :\n",
    "            \n",
    "            dp[1][i] = dp[0][i-1] * trans_mat[0][1] * emission_mat[1][sent_ord[i]]\n",
    "            path[1][i] = 0\n",
    "            \n",
    "        else :\n",
    "            dp[1][i] = dp[1][i-1] * trans_mat[1][1] * emission_mat[1][sent_ord[i]]\n",
    "            path[1][i] = 1\n",
    "            \n",
    "    \n",
    "    #  `labels`用来储存每个位置最有可能的隐状态\n",
    "    labels = [0 for _ in range(len(sent_ord))]\n",
    "    \n",
    "    #  TODO：计算labels每个位置上的值（填充labels矩阵）（1分）\n",
    "    \n",
    "    tmp_val = 0\n",
    "    \n",
    "    for i in reversed(range(length)):\n",
    "        if i == length-1 : \n",
    "            if (dp[0][i]>dp[0][i]) : \n",
    "                labels[i] = 0 \n",
    "                prev = path[0][i]\n",
    "            else:\n",
    "                labels[i] = 1\n",
    "                prev = path[1][i]\n",
    "        else : \n",
    "            labels[i] = prev\n",
    "            prev = path[prev][i]\n",
    "        \n",
    "        \n",
    "    #  根据lalels生成切分好的字符串\n",
    "    sent_split = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            sent_split += [sent_ord[idx], ord(\"/\")]\n",
    "        else:\n",
    "            sent_split += [sent_ord[idx]]\n",
    "    sent_split_str = \"\".join([chr(x) for x in sent_split])\n",
    "\n",
    "    return sent_split_str\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d795414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viterbi算法分词结果： 马逸川/是/一名/优秀/的/学生/\n"
     ]
    }
   ],
   "source": [
    "print(\"viterbi算法分词结果：\", viterbi(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcafdb",
   "metadata": {},
   "source": [
    "实现前向算法，计算该句子的概率值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_by_forward(sent_orig, start_prob, trans_mat, emission_mat):\n",
    "    \"\"\"\n",
    "    前向算法，计算输入中文句子的概率值\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 概率值\n",
    "    \"\"\"\n",
    "    \n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "    length = len(sent_ord)\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，到该位置为止的句子的概率\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: 初始位置概率的计算（1分）\n",
    "    dp[0][0], dp[1][0] = start_prob\n",
    "    dp[0][0] = dp[0][0] * emission_mat[0][sent_ord[0]]\n",
    "    dp[1][0] = dp[1][0] * emission_mat[1][sent_ord[0]]\n",
    "        \n",
    "    # TODO: 先计算其余位置的概率（填充dp矩阵），然后return概率值（1分）\n",
    "    \n",
    "    for i in range(1, length):\n",
    "        dp[0][i] = dp[0][i-1] * trans_mat[0][0] * emission_mat[0][sent_ord[i]] + dp[1][i-1] * trans_mat[1][0] * emission_mat[0][sent_ord[i]]\n",
    "        dp[1][i] = dp[0][i-1] * trans_mat[0][1] * emission_mat[1][sent_ord[i]] + dp[1][i-1] * trans_mat[1][1] * emission_mat[1][sent_ord[i]]\n",
    "                   \n",
    "    return sum([dp[i][len(sent_ord)-1] for i in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59533cd8",
   "metadata": {},
   "source": [
    "实现后向算法，计算该句子的概率值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e898306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_by_backward(sent_orig, start_prob, trans_mat, emission_mat):\n",
    "    \"\"\"\n",
    "    后向算法，计算输入中文句子的概率值\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 概率值\n",
    "    \"\"\"\n",
    "    \n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "    length = len(sent_ord)\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，从结尾到该位置为止的句子的概率\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    Row = trans_mat.shape[0]\n",
    "\n",
    "    dp[:,(length-1):] = 1                 #最后的每一个元素赋值为1\n",
    "\n",
    "    for t in reversed(range(length-1)):\n",
    "        for n in range(Row):\n",
    "            dp[n,t] = np.sum(dp[:,t+1]*trans_mat[n,:]*emission_mat[:,sent_ord[t+1]])\n",
    "\n",
    "    \n",
    "    # TODO: 先计算其余位置的概率（填充dp矩阵），然后return概率值（1分）\n",
    "    pass\n",
    "\n",
    "    return sum([dp[i][0] * start_prob[i] * emission_mat[i][sent_ord[0]] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26101d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向算法概率： 4.039035028999559e-34\n",
      "后向算法概率： 4.03903502899956e-34\n"
     ]
    }
   ],
   "source": [
    "print(\"前向算法概率：\", compute_prob_by_forward(input_sentence, start_probability, trans_matrix, emission_matrix))\n",
    "print(\"后向算法概率：\", compute_prob_by_backward(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994be6e",
   "metadata": {},
   "source": [
    "## 任务二：BPE算法用于英文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc4775",
   "metadata": {},
   "source": [
    "任务二评分标准：\n",
    "\n",
    "1. 共有7处TODO需要填写，每个TODO计1-2分，共9分，预计代码量50行；\n",
    "2. 允许自行修改、编写代码完成，对于该情况，请补充注释以便于评分，否则结果不正确将导致较多的扣分；\n",
    "3. 实验报告(python)/用于说明实验的文字块(jupyter notebook)不额外计分，但不写会导致扣分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5dbb9",
   "metadata": {},
   "source": [
    "构建空格分词器，将语料中的句子以空格切分成单词，然后将单词拆分成字母加`</w>`的形式。例如`apple`将变为`a p p l e </w>`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "70e10703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d6c3667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "def white_space_tokenize(corpus):\n",
    "    \"\"\"\n",
    "    先正则化（字母转小写、数字转为N、除去标点符号），然后以空格分词语料中的句子，例如：\n",
    "    输入 corpus=[\"I am happy.\", \"I have 10 apples!\"]，\n",
    "    得到 [[\"i\", \"am\", \"happy\"], [\"i\", \"have\", \"N\", \"apples\"]]\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        List[List[str]] - 二维List，内部的List由每个句子的单词str构成\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [list(\n",
    "        filter(lambda tkn: len(tkn)>0, _splitor_pattern.split(_digit_pattern.sub(\"N\", stc.lower())))) for stc in corpus\n",
    "    ]\n",
    "    \n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732502a",
   "metadata": {},
   "source": [
    "#### 编写相应函数构建BPE算法需要用到的初始状态词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7bf823e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bpe_vocab(corpus):\n",
    "    \"\"\"\n",
    "    将语料进行white_space_tokenize处理后，将单词每个字母以空格隔开、结尾加上</w>后，构建带频数的字典，例如：\n",
    "    输入 corpus=[\"I am happy.\", \"I have 10 apples!\"]，\n",
    "    得到\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "     }\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = white_space_tokenize(corpus)\n",
    "    # print(tokenized_corpus)\n",
    "    bpe_vocab = dict() \n",
    "    \n",
    "    # TODO: 完成函数体（1分）\n",
    "    for tokens in tokenized_corpus : \n",
    "        # print(tokens)\n",
    "        if isinstance(tokens, list):\n",
    "            for token in tokens :\n",
    "                token = list(token)\n",
    "                for i in range(1,len(token)+int(len(token))//1,2):\n",
    "                    token.insert(i, ' ')\n",
    "                \n",
    "                token.insert(len(token), '</w>')\n",
    "                \n",
    "                str = ''.join(token)\n",
    "                    \n",
    "                if str in bpe_vocab:\n",
    "                    bpe_vocab[str] = bpe_vocab[str] + 1\n",
    "                else:\n",
    "                    bpe_vocab[str] = 1\n",
    "\n",
    "    return bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52847532",
   "metadata": {},
   "source": [
    "#### test bpe_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e310b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training corpus.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding=\"utf-8\") as f:\n",
    "    training_corpus = list(map(lambda l: l.strip(), f.readlines()[:1000]))\n",
    "\n",
    "print(\"Loaded training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ff48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bpe_vocab = build_bpe_vocab(training_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d25245",
   "metadata": {},
   "source": [
    "#### 编写所需的其他函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb4dae",
   "metadata": {},
   "source": [
    "##### get_bigram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "087d11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_freq(bpe_vocab):\n",
    "    \"\"\"\n",
    "    统计\"单词分词状态->频数\"的词典中，各bigram的频次（假设该词典中，各个unigram以空格间隔），例如：\n",
    "    输入 bpe_vocab=\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    得到\n",
    "    {\n",
    "        ('i', '</w>'): 2,\n",
    "        ('a', 'm'): 1,\n",
    "        ('m', '</w>'): 1,\n",
    "        ('h', 'a'): 2,\n",
    "        ('a', 'p'): 2,\n",
    "        ('p', 'p'): 2,\n",
    "        ('p', 'y'): 1,\n",
    "        ('y', '</w>'): 1,\n",
    "        ('a', 'v'): 1,\n",
    "        ('v', 'e'): 1,\n",
    "        ('e', '</w>'): 1,\n",
    "        ('N', '</w>'): 1,\n",
    "        ('p', 'l'): 1,\n",
    "        ('l', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', '</w>'): 1\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[Tuple(str, str), int] - \"bigram->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    bigram_freq = dict()\n",
    "    \n",
    "    for token, value in bpe_vocab.items():\n",
    "        \n",
    "        token = token.split(' ')\n",
    "        # print(token)\n",
    "        \n",
    "        for i in range(len(token)-1):\n",
    "            gram = (token[i], token[i+1])\n",
    "            if gram in bigram_freq:\n",
    "                bigram_freq[gram] = bigram_freq[gram] + value\n",
    "            else:\n",
    "                bigram_freq[gram] = value\n",
    "            \n",
    "            \n",
    "    return bigram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa1bb0",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "49c8b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('i', '</w>'): 2, ('a', 'm'): 1, ('m', '</w>'): 1, ('h', 'a'): 2, ('a', 'p'): 2, ('p', 'p'): 2, ('p', 'y'): 1, ('y', '</w>'): 1, ('a', 'v'): 1, ('v', 'e'): 1, ('e', '</w>'): 1, ('N', '</w>'): 1, ('p', 'l'): 1, ('l', 'e'): 1, ('e', 's'): 1, ('s', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "bigram_frequencies = get_bigram_freq(bpe_vocab=\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    })\n",
    "\n",
    "print(bigram_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0a171",
   "metadata": {},
   "source": [
    "##### refresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ba426043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_bpe_vocab_by_merging_bigram(bigram, old_bpe_vocab):\n",
    "    \"\"\"\n",
    "    在\"单词分词状态->频数\"的词典中，合并*指定的*bigram（即去掉对应的相邻unigram之间的空格），最后返回新的词典，例如：\n",
    "    输入 bigram=('i', '</w>')，old_bpe_vocab=\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    得到\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        old_bpe_vocab: Dict[str, int] - 初始\"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - 合并后的\"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "    \n",
    "    new_bpe_vocab = dict()\n",
    "    \n",
    "    for token, value in old_bpe_vocab.items():\n",
    "        token = token.split(' ')\n",
    "\n",
    "        i=0\n",
    "        while(True):\n",
    "            # del the space if satisfying condiitons\n",
    "            # print(token[i], token[i+1]) \n",
    "            if token[i]==bigram[0] and token[i+1]==bigram[1] :\n",
    "                new_token = token[i] + token[i+1]\n",
    "                token[i] = new_token\n",
    "                del token[i+1]\n",
    "            \n",
    "            i = i + 1\n",
    "            if i>=len(token)-1:\n",
    "                break\n",
    "                \n",
    "        token = ' '.join(token)\n",
    "        # print(token)\n",
    "        new_bpe_vocab[token] = value\n",
    "        \n",
    "    return new_bpe_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c2b6b64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i </w>': 2,\n",
       " 'a m i i </w> aai </w>': 2,\n",
       " 'a m </w>': 1,\n",
       " 'h a p p y </w>': 1,\n",
       " 'h a v e </w>': 1,\n",
       " 'N </w>': 1,\n",
       " 'a p p l e s </w>': 1}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refresh_bpe_vocab_by_merging_bigram(bigram=('aa', 'i'),old_bpe_vocab=\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m i i </w> aa i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d03ec7",
   "metadata": {},
   "source": [
    "##### 由于下面多次使用字符串长度，这里封装一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d61c18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Length(str):\n",
    "    str = list(str)\n",
    "    cnt = 0 \n",
    "    i = 0 \n",
    "    \n",
    "    while(True):\n",
    "        if str[i] != '<':\n",
    "            cnt = cnt+1\n",
    "            i = i+1\n",
    "        else:\n",
    "            if i+3 <= len(str) and ''.join(str[i:i+4])=='</w>':\n",
    "                cnt = cnt+1\n",
    "                i = i+4\n",
    "            else:\n",
    "                cnt = cnt+1\n",
    "                i = i+1\n",
    "        \n",
    "        if i >= len(str):\n",
    "            break\n",
    "\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffddd9",
   "metadata": {},
   "source": [
    "##### get_bpe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "992438a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpe_tokens(bpe_vocab):\n",
    "    \"\"\"\n",
    "    根据\"单词分词状态->频数\"的词典，返回所得到的BPE分词列表，并将该列表按照分词长度降序排序返回，例如：\n",
    "    输入 bpe_vocab=\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }\n",
    "    得到\n",
    "    [\n",
    "        ('i</w>', 2),\n",
    "        ('ha', 2),\n",
    "        ('pp', 2),\n",
    "        ('a', 2),\n",
    "        ('m', 1),\n",
    "        ('</w>', 5),\n",
    "        ('y', 1),\n",
    "        ('v', 1),\n",
    "        ('e', 2),\n",
    "        ('N', 1),\n",
    "        ('l', 1),\n",
    "        ('s', 1)\n",
    "     ]\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        List[Tuple(str, int)] - BPE分词和对应频数组成的List\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: 完成函数体（2分）\n",
    "    # Reference: n-gram HW1\n",
    "        \n",
    "    tmp_vocab = {}\n",
    "    for tokens, value in bpe_vocab.items():\n",
    "        tokens = tokens.split(' ')\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in tmp_vocab:\n",
    "                tmp_vocab[token] = tmp_vocab[token] + value\n",
    "            else:\n",
    "                tmp_vocab[token] = value\n",
    "            \n",
    "    bpe_tokens = sorted(\n",
    "                sorted(\n",
    "                    map(lambda itm: (itm[0], itm[1]),  # 先按照item 1 排序\n",
    "                        tmp_vocab.items()),\n",
    "                    key=(lambda itm: itm[1]), reverse=True),\n",
    "                key=(lambda itm: Count_Length(itm[0])), reverse=True) # 再按照第一位元素排序， 后面的值为频率\n",
    "    # TODO: calculates the value of $N_r$\n",
    "    # print(grams)\n",
    "    return bpe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "a8af59fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ppy</w>', 1),\n",
       " ('i</w>', 2),\n",
       " ('ha', 2),\n",
       " ('pp', 1),\n",
       " ('</w>', 4),\n",
       " ('a', 2),\n",
       " ('e', 2),\n",
       " ('m', 1),\n",
       " ('v', 1),\n",
       " ('N', 1),\n",
       " ('l', 1),\n",
       " ('s', 1)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bpe_tokens(bpe_vocab=\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha ppy</w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca607b",
   "metadata": {},
   "source": [
    "##### 实现字符串分割函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "05b55f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Length(str):\n",
    "\n",
    "    if len(str)==0:\n",
    "        return 0\n",
    "    str = list(str)\n",
    "    cnt = 0 \n",
    "    i = 0 \n",
    "\n",
    "    \n",
    "    while(True):\n",
    "        if str[i] != '<':\n",
    "            cnt = cnt+1\n",
    "            i = i+1\n",
    "        else:\n",
    "            if i+3 <= len(str) and ''.join(str[i:i+4])=='</w>':\n",
    "                cnt = cnt+1\n",
    "                i = i+4\n",
    "            else:\n",
    "                cnt = cnt+1\n",
    "                i = i+1\n",
    "        \n",
    "        if i >= len(str):\n",
    "            break\n",
    "\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c3435bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Token(str):  # 为了将最后的符号也区分出来\n",
    "    str = list(str)\n",
    "\n",
    "    i=0\n",
    "    while(True):\n",
    "        if str[i] != '<':\n",
    "            i = i+1\n",
    "        else:\n",
    "            if i+3 <= len(str) and ''.join(str[i:i+4])=='</w>':\n",
    "                str[i] = ''.join(str[i:i+4])\n",
    "                del str[i+1:i+4]\n",
    "                i = i + 1\n",
    "            else:\n",
    "                i = i+1\n",
    "        \n",
    "        if i >= len(str):\n",
    "            break\n",
    "\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f00df0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'u', 'p', 'e', 'r', 'm', 'a', 'r', 'k', 'e', 't', '</w>']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = \"supermarket</w>\"\n",
    "Split_Token(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3005e",
   "metadata": {},
   "source": [
    "##### 对词表按长度排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "db4abe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_length(bpe_vocab):\n",
    "    sorted_list = sorted(\n",
    "                        map(lambda itm: (itm[0], itm[1]),  # 先按照item 1 排序\n",
    "                            bpe_vocab.items()),\n",
    "                        key=(lambda itm: itm[0]), reverse=True)\n",
    "    sorted_vocab = {}\n",
    "    for itm in sorted_list:\n",
    "        sorted_vocab[itm[0]] = itm[1]\n",
    "    \n",
    "    return sorted_vocab\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e65e8303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i</w>': 2,\n",
       " 'ha v e </w>': 1,\n",
       " 'ha ppp y </w>': 1,\n",
       " 'a pp l e s </w>': 1,\n",
       " 'a m </w>': 1,\n",
       " 'N </w>': 1}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sort_length( bpe_vocab=\n",
    "    { \n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha ppp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f36d4",
   "metadata": {},
   "source": [
    "##### print tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3c56995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bpe_tokenize(word, bpe_tokens):\n",
    "    \"\"\"\n",
    "    根据按长度降序的BPE分词列表，将所给单词进行BPE分词，最后打印结果。\n",
    "    \n",
    "    思想是，对于一个待BPE分词的单词，按照长度顺序从列表中寻找BPE分词进行子串匹配，\n",
    "    若成功匹配，则对该子串左右的剩余部分递归地进行下一轮匹配，直到剩余部分长度为0，\n",
    "    或者剩余部分无法匹配（该部分整体由\"<unknown>\"代替）\n",
    "    \n",
    "    例1：\n",
    "    输入 word=\"supermarket\", bpe_tokens=[\n",
    "        (\"su\", 20),\n",
    "        (\"are\", 10),\n",
    "        (\"per\", 30),\n",
    "    ]\n",
    "    最终打印 \"su per <unknown>\"\n",
    "\n",
    "    例2：\n",
    "    输入 word=\"shanghai\", bpe_tokens=[\n",
    "        (\"hai\", 1),\n",
    "        (\"sh\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1)\n",
    "    ]\n",
    "    最终打印 \"sh an g hai </w>\"\n",
    "\n",
    "    Args:\n",
    "        word: str - 待分词的单词str\n",
    "        bpe_tokens: List[Tuple(str, int)] - BPE分词和对应频数组成的List\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: 请尝试使用递归函数定义该分词过程（2分）\n",
    "     \n",
    "    def bpe_tokenize(sub_word):\n",
    "        if len(sub_word)==0:\n",
    "            return ''\n",
    "        len_word = Count_Length(sub_word)\n",
    "        word_list = Split_Token(sub_word)\n",
    "        \n",
    "        flag = False\n",
    "        \n",
    "        for token, _ in bpe_tokens:  # 按长度查询词表\n",
    "            len_token = Count_Length(token)\n",
    "            token_list = Split_Token(token)\n",
    "            for i in range(len_word): # 从左到右依次对比\n",
    "                if i+len_token-1 >=len_word:\n",
    "                    break\n",
    "                # 相等时， 向两边递归\n",
    "                \n",
    "                if word_list[i:i+len_token] == token_list : \n",
    "                    flag = True            \n",
    "                    left_word  = ''.join(word_list[0:i])\n",
    "                    right_word = ''.join(word_list[i+len_token:len_word])\n",
    "                    # print('left', left_word)\n",
    "                    # print('right', right_word)\n",
    "                    \n",
    "                    # if len(left_word)\n",
    "                    # print(token)\n",
    "                    ret_str = bpe_tokenize(left_word) + token + ' ' + bpe_tokenize(right_word)\n",
    "                    return ret_str\n",
    "        \n",
    "        if not flag:\n",
    "            return '<unknown>'\n",
    "                \n",
    "    res = bpe_tokenize(word+\"</w>\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1a287ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people are g <unknown></w> \n"
     ]
    }
   ],
   "source": [
    "print_bpe_tokenize(word=\"peoplearegood\", bpe_tokens=[\n",
    "        (\"people\", 1),\n",
    "        (\"are\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1),\n",
    "        (\"shangh\", 1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd70402",
   "metadata": {},
   "source": [
    "##### 开始读取数据集并训练BPE分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "215b56d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training corpus.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding=\"utf-8\") as f:\n",
    "    training_corpus = list(map(lambda l: l.strip(), f.readlines()[:1000]))\n",
    "\n",
    "print(\"Loaded training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7bccd41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('people</w>', 47), ('ation</w>', 106), ('their</w>', 56), ('other</w>', 50), ('would</w>', 49), ('which</w>', 49), ('ement</w>', 44), ('after</w>', 44), ('about</w>', 41), ('that</w>', 198), ('with</w>', 155), ('said</w>', 153), ('from</w>', 99), ('tion</w>', 94), ('have</w>', 92), ('they</w>', 77), ('ting</w>', 75), ('ment</w>', 75), ('will</w>', 70), ('this</w>', 69), ('ight</w>', 59), ('ents</w>', 53), ('more</w>', 52), ('ence</w>', 50), ('ated</w>', 50), ('when</w>', 47), ('onal</w>', 46), ('year</w>', 45), ('were</w>', 45), ('inter', 44), ('also</w>', 44), ('ally</w>', 42), ('ates</w>', 42), ('ound</w>', 40), ('ould</w>', 34), ('ther</w>', 27), ('the</w>', 1218), ('ing</w>', 597), ('and</w>', 552), ('for</w>', 209), ('ers</w>', 178), ('ent</w>', 142), ('was</w>', 121), ('are</w>', 104), ('ted</w>', 101), ('his</w>', 100), ('ate</w>', 95), ('day</w>', 94), ('ons</w>', 87), ('ver</w>', 73), ('has</w>', 72), ('new</w>', 70), ('but</w>', 70), ('ere</w>', 68), ('all</w>', 68), ('one</w>', 66), ('our</w>', 66), ('its</w>', 64), ('ess</w>', 62), ('ice</w>', 62), ('ity</w>', 60), ('who</w>', 57), ('not</w>', 56), ('ard</w>', 51), ('out</w>', 51), ('had</w>', 51), ('coun', 51), ('ome</w>', 50), ('ter</w>', 49), ('est</w>', 49), ('ine</w>', 49), ('you</w>', 48), ('ake</w>', 48), ('comm', 47), ('ame</w>', 45), ('ast</w>', 45), ('ber</w>', 44), ('cont', 43), ('age</w>', 42), ('man</w>', 41), ('year', 28), ('ple</w>', 11), ('ill</w>', 11), ('ich</w>', 6), ('ed</w>', 687), ('to</w>', 577), ('of</w>', 538), ('in</w>', 506), ('es</w>', 444), ('on</w>', 423), ('er</w>', 348), ('an</w>', 315), ('al</w>', 262), ('en</w>', 225), ('is</w>', 194), (\"'s</w>\", 188), ('as</w>', 163), ('pro', 156), ('at</w>', 151), ('it</w>', 150), ('ts</w>', 140), ('com', 136), ('for', 131), ('le</w>', 129), ('con', 129), ('or</w>', 128), ('ly</w>', 127), ('he</w>', 127), ('ver', 124), ('ve</w>', 123), ('st</w>', 121), ('per', 117), ('th</w>', 109), ('by</w>', 106), ('ds</w>', 101), ('ch</w>', 101), ('be</w>', 100), ('ati', 94), ('par', 92), ('ce</w>', 88), ('se</w>', 86), ('pre', 86), ('ear', 83), ('wor', 81), ('ain', 81), ('ay</w>', 79), ('ar</w>', 78), ('et</w>', 71), ('str', 70), ('ic</w>', 67), ('me</w>', 67), ('ge</w>', 66), ('por', 66), ('we</w>', 65), ('and', 65), ('rec', 64), ('mon', 62), ('mar', 62), ('mil', 59), ('ing', 58), ('sid', 58), ('ent', 57), ('oug', 56), ('el</w>', 54), ('ser', 54), ('off', 52), ('oun', 51), ('sur', 51), ('end', 50), ('tic', 49), ('fir', 49), ('man', 48), ('igh', 47), ('min', 47), ('ter', 47), ('tur', 47), ('und', 46), (\"'t</w>\", 46), ('our', 45), ('car', 45), ('est', 44), ('em</w>', 44), ('am</w>', 43), ('all', 43), ('ist', 43), ('ss</w>', 43), ('ks</w>', 41), ('ad</w>', 41), ('ty</w>', 41), ('pol', 41), ('us</w>', 40), ('id</w>', 35), ('ir</w>', 25), ('so</w>', 24), ('mor', 22), ('om</w>', 16), ('ld</w>', 15), ('the', 8), ('you', 4), ('peo', 1), ('N</w>', 779), ('a</w>', 746), ('e</w>', 745), ('s</w>', 640), ('re', 565), ('in', 536), ('y</w>', 522), ('an', 498), ('t</w>', 487), ('th', 373), ('st', 364), ('ar', 363), ('en', 356), ('or', 317), ('ti', 315), ('al', 303), ('el', 300), ('er', 294), ('il', 283), ('ch', 283), ('ic', 283), ('as', 277), ('is', 268), ('ac', 265), ('ec', 260), ('d</w>', 246), ('on', 240), ('k</w>', 235), ('sh', 220), ('at', 218), ('am', 210), ('ol', 203), ('ro', 197), ('o</w>', 192), ('ri', 191), ('it', 190), ('es', 185), ('ur', 184), ('le', 180), ('un', 180), ('ad', 179), ('lo', 179), ('us', 178), ('li', 171), ('pl', 165), ('de', 162), ('em', 159), ('n</w>', 155), ('p</w>', 154), ('i</w>', 149), ('w</w>', 145), ('ou', 143), ('be', 137), ('su', 136), ('se', 136), ('tr', 132), ('om', 131), ('ag', 130), ('di', 130), ('ex', 129), ('si', 128), ('l</w>', 124), ('ne', 122), ('ir', 120), ('ap', 120), ('po', 120), ('ig', 119), ('ab', 116), ('mo', 103), ('f</w>', 101), ('to', 101), ('wh', 99), ('m</w>', 97), ('vi', 94), ('h</w>', 92), ('ul', 91), ('no', 88), ('sa', 85), ('sp', 85), ('qu', 84), ('ow', 81), ('g</w>', 77), ('op', 76), ('ed', 75), ('te', 73), ('uc', 71), ('ud', 70), ('gr', 66), ('cl', 66), ('ut', 65), ('bo', 65), ('bl', 63), ('go', 63), ('oc', 62), ('ea', 61), ('ak', 60), ('we', 59), ('ay', 58), ('im', 58), ('um', 56), ('sc', 55), ('bu', 55), ('pe', 53), ('fe', 52), ('pr', 51), ('ha', 49), ('dr', 48), ('id', 48), ('me', 48), ('av', 46), ('gi', 46), ('ff', 45), ('co', 44), ('au', 42), ('x</w>', 41), ('ho', 40), ('ra', 40), ('fr', 36), ('af', 26), ('wi', 24), ('c', 933), ('s', 832), ('t', 776), ('i', 726), ('d', 719), ('b', 692), ('p', 665), ('h', 659), ('e', 650), ('f', 649), ('m', 623), ('g', 577), ('o', 564), ('l', 551), ('v', 527), ('a', 526), ('w', 524), ('u', 509), ('r', 481), ('n', 454), ('k', 397), ('</w>', 379), ('y', 264), ('j', 217), ('z', 111), (\"'\", 69), ('x', 42), ('N', 38), ('q', 38)]\n"
     ]
    }
   ],
   "source": [
    "training_iter_num = 300\n",
    "\n",
    "training_bpe_vocab = build_bpe_vocab(training_corpus)\n",
    "# 思路：合并最高频bigram\n",
    "for i in range(training_iter_num):\n",
    "\n",
    "    # TODO: 完成训练循环内的代码逻辑（2分）\n",
    "    # if i == 10:\n",
    "    #     print(training_bpe_vocab)\n",
    "    #     break\n",
    "    bigram_freq = get_bigram_freq(training_bpe_vocab)\n",
    "    # print(bigram_freq)\n",
    "    most_freq_gram = sorted(map(lambda itm: (itm[0], itm[1]),  # 先按照item 1 排序\n",
    "                        bigram_freq.items()), key=(lambda itm: itm[1]), reverse=True)[0][0]\n",
    "    # print(most_freq_gram)\n",
    "    training_bpe_vocab = refresh_bpe_vocab_by_merging_bigram(most_freq_gram, training_bpe_vocab)\n",
    "    training_bpe_vocab = Sort_length(training_bpe_vocab)\n",
    "    \n",
    "                \n",
    " \n",
    "training_bpe_tokens = get_bpe_tokens(training_bpe_vocab)\n",
    "print(training_bpe_tokens )   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea3ddd",
   "metadata": {},
   "source": [
    "测试BPE分词器的分词效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "c0cfdb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naturallanguageprocessing 的分词结果为：\n",
      "n a tur all an g u ag e pro c es s ing</w> \n"
     ]
    }
   ],
   "source": [
    "test_word = \"naturallanguageprocessing\"\n",
    "\n",
    "print(\"naturallanguageprocessing 的分词结果为：\")\n",
    "print_bpe_tokenize(test_word, training_bpe_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4be44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6800e12ede7b2864e9122d834c67bc469ffc126bc5bf32ee6e2d901e32bee8d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
