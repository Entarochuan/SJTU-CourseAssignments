{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ace494b",
   "metadata": {},
   "source": [
    "# n元语言模型回退算法\n",
    "\n",
    "本次作业要求补全本笔记中的n元语言模型的采用Good-Turing折扣的Katz回退算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d9726",
   "metadata": {},
   "source": [
    "### 预处理\n",
    "\n",
    "首先创建一些预处理函数。\n",
    "\n",
    "引入必要的模块，定义些类型别名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0097797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "Sentence = List[str]\n",
    "IntSentence = List[int]\n",
    "\n",
    "Corpus = List[Sentence]\n",
    "IntCorpus = List[IntSentence]\n",
    "\n",
    "Gram = Tuple[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c02038",
   "metadata": {},
   "source": [
    "下面的函数用于将文本正则化并词元化。该函数会将所有英文文本转为小写，去除文本中所有的标点，简单起见将所有连续的数字用一个`N`代替，将形如`let's`的词组拆分为`let`和`'s`两个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd05065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "def normaltokenize(corpus: List[str]) -> Corpus:\n",
    "    \"\"\"\n",
    "    Normalizes and tokenizes the sentences in `corpus`. Turns the letters into\n",
    "    lower case and removes all the non-alphadigit characters and splits the\n",
    "    sentence into words and added BOS and EOS marks.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of str\n",
    "\n",
    "    Return:\n",
    "        list of list of str where each inner list of str represents the word\n",
    "          sequence in a sentence from the original sentence list\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [ [\"<s>\"]\n",
    "               + list(\n",
    "                   filter(lambda tkn: len(tkn)>0,\n",
    "                       _splitor_pattern.split(\n",
    "                           _digit_pattern.sub(\"N\", stc.lower()))))\n",
    "               + [\"</s>\"]\n",
    "                    for stc in corpus\n",
    "               ]\n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2768c1",
   "metadata": {},
   "source": [
    "接下来定义两个函数用来从训练语料中构建词表，并将句子中的单词从字符串表示转为整数索引表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4685897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(corpus: Corpus) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary from `corpus` and returns it as a mapping from the\n",
    "    word to index. The words will be sorted by the codepoint value.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of list of str\n",
    "\n",
    "    Return:\n",
    "        dict like {str: int}\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "    vocabulary = dict(\n",
    "            map(lambda itm: (itm[1], itm[0]),\n",
    "                enumerate(\n",
    "                    sorted(vocabulary))))\n",
    "    return vocabulary\n",
    "\n",
    "def words_to_indices(vocabulary: Dict[str, int], sentence: Sentence) -> IntSentence:\n",
    "    \"\"\"\n",
    "    Convert sentence in words to sentence in word indices.\n",
    "\n",
    "    Args:\n",
    "        vocabulary - dict like {str: int}\n",
    "        sentence - list of str\n",
    "\n",
    "    Return:\n",
    "        list of int\n",
    "    \"\"\"\n",
    "\n",
    "    return list(map(lambda tkn: vocabulary.get(tkn, len(vocabulary)), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265aba2b",
   "metadata": {},
   "source": [
    "接下来读入训练数据，将数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19af69f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set.\n",
      "Preprocessed training set.\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding='UTF-8') as f:\n",
    "    texts = list(map(lambda l: l.strip(), f.readlines()))\n",
    "\n",
    "print(\"Loaded training set.\")\n",
    "\n",
    "corpus = normaltokenize(texts)\n",
    "vocabulary = extract_vocabulary(corpus)\n",
    "corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            corpus))\n",
    "\n",
    "print(\"Preprocessed training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb04393d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 32\n",
      "[3581, 165570, 86217, 62968, 165570, 34971, 167440, 82721, 178887, 132056, 12791, 165570, 142662, 142568, 6618, 134164, 176515, 9880, 131488, 7782, 99960, 165570, 169869, 80313, 3582, 9880, 165570, 109285, 8702, 144231, 119677, 165570, 68036, 80313, 3582, 3580]\n",
      "[3581, 165570, 178819, 62968, 165570, 58547, 180493, 41355, 82173, 135885, 51060, 166148, 179627, 169773, 36804, 165541, 83663, 106975, 76095, 118831, 63602, 135877, 41396, 120257, 55961, 37434, 135208, 165710, 79221, 81152, 5006, 3580]\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus[0]), len(corpus[1]))\n",
    "\n",
    "print(corpus[0])\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007b899",
   "metadata": {},
   "source": [
    "### 设计模型\n",
    "\n",
    "参照公式\n",
    "\n",
    "$$\n",
    "P_{\\text{bo}}(w_k | W_{k-n+1}^{k-1}) = \\begin{cases}\n",
    "    d(W_{k-n+1}^k) \\dfrac{C(W_{k-n+1}^k)}{C(W_{k-n+1}^{k-1})} &  C(W_{k-n+1}^k) > 0 \\\\\n",
    "    \\alpha(W_{k-n+1}^{k-1}) P_{\\text{bo}}(w_k | W_{k-n+2}^{k-1}) &  \\text{否则} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "实现n元语言模型及采用Good-Turing折扣的Katz回退算法。\n",
    "\n",
    "需要实现的功能包括：\n",
    "\n",
    "1. 统计各词组（gram）在训练语料中的频数\n",
    "2. 计算同频词组个数$N_r$\n",
    "3. 计算$d(W_{k-n+1}^k)$\n",
    "4. 计算$\\alpha(W_{k-n+1}^{k-1})$\n",
    "5. 根据公式计算回退概率\n",
    "6. 计算概率对数与困惑度（PPL）\n",
    "\n",
    "$d$与$\\alpha$如何计算可以参考作业文件中的算法说明以及[SRILM](http://www.speech.sri.com/projects/srilm/)的[`ngram-discount(7)`手册页](http://www.speech.sri.com/projects/srilm/manpages/ngram-discount.7.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea6708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, vocab_size: int, n: int = 4):\n",
    "        \"\"\"\n",
    "        Constructs `n`-gram model with a `vocab_size`-size vocabulary.\n",
    "\n",
    "        Args:\n",
    "            vocab_size - int\n",
    "            n - int\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab_size: int = vocab_size\n",
    "        self.n: int = n\n",
    "\n",
    "        self.frequencies: List[Dict[Gram, int]] \\\n",
    "            = [{} for _ in range(n)]\n",
    "\n",
    "        # 我认为应该更改为float 10/20 11:17\n",
    "        self.disfrequencies: List[Dict[Gram, float]] \\\n",
    "            = [{} for _ in range(n)]\n",
    "\n",
    "        self.ncounts: Dict[Gram\n",
    "        , Dict[int, int]\n",
    "        ] = {}\n",
    "\n",
    "        self.discount_threshold: int = 7\n",
    "        self._d: Dict[Gram, Tuple[float, float]] = {}\n",
    "        self._alpha: List[Dict[Gram, float]] \\\n",
    "            = [{} for _ in range(n)]\n",
    "\n",
    "        self.eps = 1e-10\n",
    "\n",
    "    def learn(self, corpus: IntCorpus):\n",
    "        \"\"\"\n",
    "        Learns the parameters of the n-gram model.\n",
    "\n",
    "        Args:\n",
    "            corpus - list of list of int\n",
    "        \"\"\"\n",
    "\n",
    "        # self.n: lenth of n-gram\n",
    "        for stc in corpus:\n",
    "            # print(stc)\n",
    "            for i in range(1, len(stc) + 1):  # 查看到i为止的序列\n",
    "                for j in range(min(i, self.n)):  # 以j作为gram的迭代\n",
    "                    # TODO: count the frequencies of the grams\n",
    "                    tmp_gram = stc[i - j - 1:i]\n",
    "                    tmp_gram = tuple(tmp_gram)\n",
    "                    if self.frequencies[j].get(tmp_gram, 0):\n",
    "                        self.frequencies[j][tmp_gram] = self.frequencies[j][tmp_gram] + 1\n",
    "\n",
    "                        # print(self.frequencies[j][tmp_gram])\n",
    "                    else:\n",
    "                        self.frequencies[j][tmp_gram] = 1\n",
    "\n",
    "                    # print(self.frequencies[j][tmp_gram])\n",
    "        # print(self.frequencies[1])\n",
    "\n",
    "        for i in range(1, self.n):  # 按照gram长度划分\n",
    "            grams = itertools.groupby(\n",
    "                sorted(\n",
    "                    sorted(\n",
    "                        map(lambda itm: (itm[0][:-1], itm[1]),  # 去除最后一项后的键值对\n",
    "                            self.frequencies[i].items()),\n",
    "                        key=(lambda itm: itm[1])),\n",
    "                    key=(lambda itm: itm[0])))  # 再按照第一位元素排序， 后面的值为频率\n",
    "            # TODO: calculates the value of $N_r$\n",
    "\n",
    "            for key, group in grams:  # key[0]: 表示上一项的值, key[1]: 下一项出现频次\n",
    "\n",
    "                cnt = 0\n",
    "                for _ in group:\n",
    "                    cnt = cnt + 1\n",
    "\n",
    "                if key[0] in self.ncounts:\n",
    "                    self.ncounts[key[0]][key[1]] = cnt  # 上一长度gram: (多一个元素所有可能的频次: 该频次的出现次数)\n",
    "                else:\n",
    "                    self.ncounts[key[0]] = {}\n",
    "                    self.ncounts[key[0]][key[1]] = cnt\n",
    "\n",
    "                # 记录了: 对本项(key)，可能的下一项的(r , N_r)\n",
    "\n",
    "        return self.frequencies\n",
    "\n",
    "    # 输入gram,计算其对应插值参数d\n",
    "    def d(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the interpolation coefficient.\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "        # r即本gram的出现频次\n",
    "        length = len(gram)\n",
    "        r = self.frequencies[length].get(gram, self.eps)\n",
    "\n",
    "        # 调用self.ncounts， 查看下一项的不同频次的出现次数\n",
    "        if gram not in self._d:\n",
    "            # print(gram)\n",
    "            # print(gram[:-1])\n",
    "            # TODO: calculates the value of $d'$\n",
    "            ncounts = self.ncounts[gram[:-1]]  # 查看前缀, counts是存储频次的字典\n",
    "            # print(ncounts)\n",
    "\n",
    "            def counts(num):\n",
    "                if num in ncounts:\n",
    "                    return float(ncounts[num])\n",
    "                else:\n",
    "                    return float(self.eps)  # 如有报错,改为0\n",
    "\n",
    "            lamda = float(counts(1) / (counts(1) - 8 * counts(8)))\n",
    "            N_r = counts(r)\n",
    "            N_r_plus1 = counts(r + 1)\n",
    "            d_dot = lamda * ((r + 1) * N_r_plus1) / (r * N_r) + 1 - lamda  # 求出各项参数 及d'\n",
    "\n",
    "            self._d[gram] = (d_dot, 1.0)\n",
    "\n",
    "        # self._d[gram] = (numerator1 / denominator, - numerator2 / denominator)  # 10/19 ques ?\n",
    "\n",
    "        if r > 7:\n",
    "            return self._d[gram][1]\n",
    "        else:\n",
    "            return self._d[gram][0]\n",
    "\n",
    "    def alpha(self, gram: Gram) -> float:  # '回退' 意为降低gram大小要求\n",
    "        \"\"\"\n",
    "        Calculates the back-off weight alpha(`gram`)\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)\n",
    "        # 概率存放在 disfrequencies 内\n",
    "        if gram not in self._alpha[n]:\n",
    "            if gram in self.frequencies[n - 1]:\n",
    "                # TODO: calculates the value of $\\alpha$\n",
    "\n",
    "                # V+, V- Accumulated, sum add\n",
    "                V_plus = []\n",
    "                V_minus = []\n",
    "\n",
    "                sum_pls = 0\n",
    "                sum_minus = 0\n",
    "                for i in range(1, self.vocab_size):\n",
    "                    # 添加元素:查询V+和V-\n",
    "                    index = len(gram)  # 减一是原长度 checked in test_funcs 10/19 21:35\n",
    "                    gram_add = list(gram)\n",
    "                    gram_add.append(i)\n",
    "                    gram_add = tuple(gram_add)\n",
    "\n",
    "                    if gram_add in self.frequencies[index]:\n",
    "                        # print(gram_add)\n",
    "                        # print(self.frequencies[index].get(gram_add, 0))\n",
    "                        V_plus.append(i)\n",
    "                        sum_pls = sum_pls + self.disfrequencies[index].get(gram_add, 0)  # checked in test\n",
    "                        # sum_pls = sum_pls + self.disfrequencies[index].get(gram_add, 0)\n",
    "                    else:\n",
    "                        V_minus.append(i)\n",
    "                        gram_minus = gram_add[1:]\n",
    "                        index = index - 1\n",
    "\n",
    "                        sum_minus = sum_minus + self.disfrequencies[index - 1].get(gram_minus, self.eps)\n",
    "\n",
    "                numerator = 1 - sum_pls\n",
    "                denominator = 1 - sum_minus\n",
    "\n",
    "                self._alpha[n][gram] = numerator / denominator\n",
    "            else:\n",
    "                self._alpha[n][gram] = 1.\n",
    "        return self._alpha[n][gram]\n",
    "\n",
    "    def __getitem__(self, gram: Gram) -> float:  # 计算回退概率(调用之前实现的函数)\n",
    "        \"\"\"\n",
    "        Calculates smoothed conditional probability P(`gram[-1]`|`gram[:-1]`).\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram) - 1\n",
    "\n",
    "        if gram not in self.disfrequencies[n]:\n",
    "            if n > 0:\n",
    "                # TODO: calculates the smoothed probability value according to the formulate\n",
    "\n",
    "                # 1:C>0, use param_d\n",
    "                if self.disfrequencies[n].get(gram, 0) > 0:\n",
    "                    param_d = self.d(gram)\n",
    "                    # print('param_d = ', param_d)\n",
    "                    Ck = self.frequencies[n][gram]\n",
    "                    Ck_1 = self.frequencies[n - 1][gram[:-1]]\n",
    "                    P = param_d * Ck / Ck_1\n",
    "                    self.disfrequencies[n][gram] = P\n",
    "\n",
    "                # 2:C=0, use param_a\n",
    "                else:\n",
    "                    param_a = self.alpha(gram[:-1])\n",
    "                    # print('param_a = ', param_a)\n",
    "                    # P = self.disfrequencies[n-1].get(gram[1:], self.eps) * param_a\n",
    "                    P = self.__getitem__(gram[1:]) * param_a\n",
    "                    self.disfrequencies[n][gram] = P\n",
    "\n",
    "            # 第一项\n",
    "            else:\n",
    "                self.disfrequencies[n][gram] = self.frequencies[n].get(gram, self.eps) / float(len(self.frequencies[0]))\n",
    "        return self.disfrequencies[n][gram]\n",
    "\n",
    "    # 交叉熵\n",
    "    def log_prob(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the log probability of the given sentence. Assumes that the\n",
    "        first token is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        log_prob = 0.\n",
    "        cnt = 0\n",
    "        n = self.n\n",
    "        for i in range(2, len(sentence) + 1):\n",
    "            # TODO: calculates the log probability\n",
    "            # 思路是递进地对每一个ngram求概率\n",
    "            cnt = float(cnt+1)\n",
    "            length = min(n, i)\n",
    "            gram = tuple(sentence[i-length:i])  # 可能超出范围,有待测试\n",
    "            # print(len(gram))\n",
    "            # print(gram)\n",
    "            log_prob = log_prob + math.log2(self.__getitem__(gram))\n",
    "\n",
    "        log_prob = - log_prob / cnt\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "    def ppl(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the PPL of the given sentence. Assumes that the first token\n",
    "        is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        # calculates the PPL\n",
    "\n",
    "        PPL = 1.\n",
    "        cnt = 0\n",
    "        n = self.n\n",
    "        for i in range(2, len(sentence) + 1):\n",
    "            # calculates the log probability\n",
    "            cnt = cnt + 1\n",
    "            length = min(n, i)\n",
    "            gram = tuple(sentence[i - length :i])  # 可能超出范围,有待测试\n",
    "            PPL = PPL / self.__getitem__(gram)\n",
    "\n",
    "        # print(PPL)\n",
    "        # print(cnt)\n",
    "\n",
    "        PPL = math.pow(PPL, 1/cnt)\n",
    "        return PPL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbcf678",
   "metadata": {},
   "source": [
    "### 训练与测试\n",
    "\n",
    "现在数据与模型均已齐备，可以训练并测试了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd706656",
   "metadata": {},
   "source": [
    "训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db5f988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped model.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "model = NGramModel(len(vocabulary))\n",
    "model.learn(corpus)\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pkl.dump(vocabulary, f)\n",
    "    pkl.dump(model, f)\n",
    "\n",
    "print(\"Dumped model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f97ca6",
   "metadata": {},
   "source": [
    "在测试集上测试计算困惑度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaf8b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.\n",
      "Avg of ppls:  74.96122718142266\n"
     ]
    }
   ],
   "source": [
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    vocabulary = pkl.load(f)\n",
    "    model = pkl.load(f)\n",
    "print(\"Loaded model.\")\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.test\") as f:\n",
    "    test_set = list(map(lambda l: l.strip(), f.readlines()))\n",
    "test_corpus = normaltokenize(test_set)\n",
    "test_corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            test_corpus))\n",
    "ppls = []\n",
    "for t in test_corpus:\n",
    "    ppls.append(model.ppl(t))\n",
    "#     print(ppls[-1])\n",
    "print(\"Avg of ppls: \", sum(ppls)/len(ppls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6151a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg of probs:  4.00465082149174\n"
     ]
    }
   ],
   "source": [
    "# Left = len(test_corpus)\n",
    "# Sum = 0\n",
    "probs = []\n",
    "for t in test_corpus:\n",
    "#     Sum = Sum + 1\n",
    "#     Left = Left - 1\n",
    "#     print(Left, \" Sentences left\")\n",
    "#     print(Sum, \" Sentences done\")\n",
    "    # print(t)\n",
    "    probs.append(model.log_prob(t))\n",
    "#     print(probs[-1])\n",
    "print(\"Avg of probs: \", sum(probs) / len(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1dcc98",
   "metadata": {},
   "source": [
    "# Ngram Report\n",
    "\n",
    "\n",
    "\n",
    "### 一、模型参数\n",
    "\n",
    "模型主要的参数包括以下几项:\n",
    "\n",
    "```python\n",
    "self.frequencies: List[Dict[Gram, int]] \\\n",
    "            = [{} for _ in range(n)]\n",
    "            \n",
    "self.disfrequencies: List[Dict[Gram, float]] \\\n",
    "            = [{} for _ in range(n)]\n",
    "            \n",
    "self.ncounts: Dict[Gram\n",
    "        , Dict[int, int]] = {}\n",
    "\n",
    "self._d: Dict[Gram, Tuple[float, float]] = {}\n",
    "\n",
    "self._alpha: List[Dict[Gram, float]] \\\n",
    "            = [{} for _ in range(n)]\n",
    "            \n",
    "```\n",
    "\n",
    "下面逐一解析各项参数的含义：\n",
    "\n",
    "`self.frequencies` : 统计并存放所有gram的出现频次。\n",
    "\n",
    "`self.disfrequencies` : 存储所有gram的出现概率(在`__getitem__`中被计算)\n",
    "\n",
    "`self.ncounts` : 以gram作为键，对应值表示以此gram为前缀的下一个gram的频次，及不同频次对应的出现次数。\n",
    "\n",
    "`self._d`: 回退算法中的d。\n",
    "\n",
    "`self._alpha`: 回退算法中的alpha\n",
    "\n",
    "注意到，`self.ncounts`参数的计算是以不同前缀gram作为前提的。（感谢助教老师的提醒）\n",
    "\n",
    "\n",
    "\n",
    "### 二、代码实现\n",
    "\n",
    "具体的实现见以上函数，下面仅简要概述各个函数的实现思路。\n",
    "\n",
    "\n",
    "\n",
    "##### 1. learn\n",
    "\n",
    "`learn`函数第一部分实现的是词频统计。\n",
    "\n",
    "在第二部分中，首先以下的函数段实现了按前缀和频次的排序。\n",
    "\n",
    "```python\n",
    "grams = itertools.groupby(\n",
    "                   sorted(\n",
    "                   sorted(\n",
    "                   map(lambda itm: (itm[0][:-1], itm[1]),  # 去除最后一项后的键值对\n",
    "                       self.frequencies[i].items()),\n",
    "                       key=(lambda itm: itm[1])),\n",
    "                       key=(lambda itm: itm[0])))  # 再按照第一位元素排序， 后面的值为频率\n",
    "```\n",
    "\n",
    "基于此，以gram作为前缀索引，即可统计不同频次的gram出现次数的值。\n",
    "\n",
    "\n",
    "\n",
    "##### 2. _d\n",
    "\n",
    "`_d`函数实现了Katz回退平滑算法中参数d的计算。调用`learn`函数中统计的`self.ncounts`，按公式计算即可。\n",
    "\n",
    "\n",
    "\n",
    "##### 3. _alpha\n",
    "\n",
    "`_alpha`函数实现了Katz回退平滑算法中参数α的计算。在实际的参数计算中，需要调用`__get_item__`函数以得到所取gram的对应概率。实际实现参照公式即可。\n",
    "\n",
    "\n",
    "\n",
    "##### 4.  __get_item__\n",
    "\n",
    "`__get_item__`函数实现了回退概率的计算。按照算法要求对输入gram进行分类，选择调用参数d和参数α计算概率，记录在`self.disfrequencies`中。\n",
    "\n",
    "\n",
    "\n",
    "##### 5. log_prob\n",
    "\n",
    "`log_prob`函数计算输入句子的概率对数。按照公式计算**log2(P(gram))**之和，取负除以cnt即可。\n",
    "\n",
    "\n",
    "\n",
    "##### 6. ppl\n",
    "\n",
    "`ppl`函数计算困惑度。同样按照公式计算即可。\n",
    "\n",
    "\n",
    "\n",
    "### 三、结果\n",
    "\n",
    "在测试集上，模型表现如下:\n",
    "\n",
    "```python\n",
    "Avg_of_probs = 4.00465082149174\n",
    "Avg_of_ppls  = 74.96187670395679\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e95f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
