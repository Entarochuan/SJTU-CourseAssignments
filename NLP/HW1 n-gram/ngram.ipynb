{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ace494b",
   "metadata": {},
   "source": [
    "# n元语言模型回退算法\n",
    "\n",
    "本次作业要求补全本笔记中的n元语言模型的采用Good-Turing折扣的Katz回退算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d9726",
   "metadata": {},
   "source": [
    "### 预处理\n",
    "\n",
    "首先创建一些预处理函数。\n",
    "\n",
    "引入必要的模块，定义些类型别名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0097797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "Sentence = List[str]\n",
    "IntSentence = List[int]\n",
    "\n",
    "Corpus = List[Sentence]\n",
    "IntCorpus = List[IntSentence]\n",
    "\n",
    "Gram = Tuple[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c02038",
   "metadata": {},
   "source": [
    "下面的函数用于将文本正则化并词元化。该函数会将所有英文文本转为小写，去除文本中所有的标点，简单起见将所有连续的数字用一个`N`代替，将形如`let's`的词组拆分为`let`和`'s`两个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd05065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "def normaltokenize(corpus: List[str]) -> Corpus:\n",
    "    \"\"\"\n",
    "    Normalizes and tokenizes the sentences in `corpus`. Turns the letters into\n",
    "    lower case and removes all the non-alphadigit characters and splits the\n",
    "    sentence into words and added BOS and EOS marks.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of str\n",
    "\n",
    "    Return:\n",
    "        list of list of str where each inner list of str represents the word\n",
    "          sequence in a sentence from the original sentence list\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [ [\"<s>\"]\n",
    "               + list(\n",
    "                   filter(lambda tkn: len(tkn)>0,\n",
    "                       _splitor_pattern.split(\n",
    "                           _digit_pattern.sub(\"N\", stc.lower()))))\n",
    "               + [\"</s>\"]\n",
    "                    for stc in corpus\n",
    "               ]\n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2768c1",
   "metadata": {},
   "source": [
    "接下来定义两个函数用来从训练语料中构建词表，并将句子中的单词从字符串表示转为整数索引表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4685897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(corpus: Corpus) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary from `corpus` and returns it as a mapping from the\n",
    "    word to index. The words will be sorted by the codepoint value.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of list of str\n",
    "\n",
    "    Return:\n",
    "        dict like {str: int}\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "    vocabulary = dict(\n",
    "            map(lambda itm: (itm[1], itm[0]),\n",
    "                enumerate(\n",
    "                    sorted(vocabulary))))\n",
    "    return vocabulary\n",
    "\n",
    "def words_to_indices(vocabulary: Dict[str, int], sentence: Sentence) -> IntSentence:\n",
    "    \"\"\"\n",
    "    Convert sentence in words to sentence in word indices.\n",
    "\n",
    "    Args:\n",
    "        vocabulary - dict like {str: int}\n",
    "        sentence - list of str\n",
    "\n",
    "    Return:\n",
    "        list of int\n",
    "    \"\"\"\n",
    "\n",
    "    return list(map(lambda tkn: vocabulary.get(tkn, len(vocabulary)), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265aba2b",
   "metadata": {},
   "source": [
    "接下来读入训练数据，将数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19af69f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set.\n",
      "Preprocessed training set.\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding='UTF-8') as f:\n",
    "    texts = list(map(lambda l: l.strip(), f.readlines()))\n",
    "\n",
    "print(\"Loaded training set.\")\n",
    "\n",
    "corpus = normaltokenize(texts)\n",
    "vocabulary = extract_vocabulary(corpus)\n",
    "corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            corpus))\n",
    "\n",
    "print(\"Preprocessed training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb04393d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 32\n",
      "[3581, 165570, 86217, 62968, 165570, 34971, 167440, 82721, 178887, 132056, 12791, 165570, 142662, 142568, 6618, 134164, 176515, 9880, 131488, 7782, 99960, 165570, 169869, 80313, 3582, 9880, 165570, 109285, 8702, 144231, 119677, 165570, 68036, 80313, 3582, 3580]\n",
      "[3581, 165570, 178819, 62968, 165570, 58547, 180493, 41355, 82173, 135885, 51060, 166148, 179627, 169773, 36804, 165541, 83663, 106975, 76095, 118831, 63602, 135877, 41396, 120257, 55961, 37434, 135208, 165710, 79221, 81152, 5006, 3580]\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus[0]), len(corpus[1]))\n",
    "\n",
    "print(corpus[0])\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007b899",
   "metadata": {},
   "source": [
    "### 设计模型\n",
    "\n",
    "参照公式\n",
    "\n",
    "$$\n",
    "P_{\\text{bo}}(w_k | W_{k-n+1}^{k-1}) = \\begin{cases}\n",
    "    d(W_{k-n+1}^k) \\dfrac{C(W_{k-n+1}^k)}{C(W_{k-n+1}^{k-1})} &  C(W_{k-n+1}^k) > 0 \\\\\n",
    "    \\alpha(W_{k-n+1}^{k-1}) P_{\\text{bo}}(w_k | W_{k-n+2}^{k-1}) &  \\text{否则} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "实现n元语言模型及采用Good-Turing折扣的Katz回退算法。\n",
    "\n",
    "需要实现的功能包括：\n",
    "\n",
    "1. 统计各词组（gram）在训练语料中的频数\n",
    "2. 计算同频词组个数$N_r$\n",
    "3. 计算$d(W_{k-n+1}^k)$\n",
    "4. 计算$\\alpha(W_{k-n+1}^{k-1})$\n",
    "5. 根据公式计算回退概率\n",
    "6. 计算概率对数与困惑度（PPL）\n",
    "\n",
    "$d$与$\\alpha$如何计算可以参考作业文件中的算法说明以及[SRILM](http://www.speech.sri.com/projects/srilm/)的[`ngram-discount(7)`手册页](http://www.speech.sri.com/projects/srilm/manpages/ngram-discount.7.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea6708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, vocab_size: int, n: int = 4):\n",
    "        \"\"\"\n",
    "        Constructs `n`-gram model with a `vocab_size`-size vocabulary.\n",
    "\n",
    "        Args:\n",
    "            vocab_size - int\n",
    "            n - int\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab_size: int = vocab_size\n",
    "        self.n: int = n\n",
    "\n",
    "        self.frequencies: List[Dict[Gram, int]] \\\n",
    "            = [{} for _ in range(n)]\n",
    "\n",
    "        # 我认为应该更改为float 10/20 11:17\n",
    "        self.disfrequencies: List[Dict[Gram, float]] \\\n",
    "            = [{} for _ in range(n)]\n",
    "\n",
    "        self.ncounts: Dict[Gram\n",
    "        , Dict[int, int]\n",
    "        ] = {}\n",
    "\n",
    "        self.discount_threshold: int = 7\n",
    "        self._d: Dict[Gram, Tuple[float, float]] = {}\n",
    "        self._alpha: List[Dict[Gram, float]] \\\n",
    "            = [{} for _ in range(n)]\n",
    "\n",
    "        self.eps = 1e-10\n",
    "\n",
    "    def learn(self, corpus: IntCorpus):\n",
    "        \"\"\"\n",
    "        Learns the parameters of the n-gram model.\n",
    "\n",
    "        Args:\n",
    "            corpus - list of list of int\n",
    "        \"\"\"\n",
    "\n",
    "        # self.n: lenth of n-gram\n",
    "        for stc in corpus:\n",
    "            # print(stc)\n",
    "            for i in range(1, len(stc) + 1):  # 查看到i为止的序列\n",
    "                for j in range(min(i, self.n)):  # 以j作为gram的迭代\n",
    "                    # TODO: count the frequencies of the grams\n",
    "                    tmp_gram = stc[i - j - 1:i]\n",
    "                    tmp_gram = tuple(tmp_gram)\n",
    "                    if self.frequencies[j].get(tmp_gram, 0):\n",
    "                        self.frequencies[j][tmp_gram] = self.frequencies[j][tmp_gram] + 1\n",
    "\n",
    "                        # print(self.frequencies[j][tmp_gram])\n",
    "                    else:\n",
    "                        self.frequencies[j][tmp_gram] = 1\n",
    "\n",
    "                    # print(self.frequencies[j][tmp_gram])\n",
    "        # print(self.frequencies[1])\n",
    "\n",
    "        for i in range(1, self.n):  # 按照gram长度划分\n",
    "            grams = itertools.groupby(\n",
    "                sorted(\n",
    "                    sorted(\n",
    "                        map(lambda itm: (itm[0][:-1], itm[1]),  # 去除最后一项后的键值对\n",
    "                            self.frequencies[i].items()),\n",
    "                        key=(lambda itm: itm[1])),\n",
    "                    key=(lambda itm: itm[0])))  # 再按照第一位元素排序， 后面的值为频率\n",
    "            # TODO: calculates the value of $N_r$\n",
    "\n",
    "            for key, group in grams:  # key[0]: 表示上一项的值, key[1]: 下一项出现频次\n",
    "\n",
    "                cnt = 0\n",
    "                for _ in group:\n",
    "                    cnt = cnt + 1\n",
    "\n",
    "                if key[0] in self.ncounts:\n",
    "                    self.ncounts[key[0]][key[1]] = cnt  # 上一长度gram: (多一个元素所有可能的频次: 该频次的出现次数)\n",
    "                else:\n",
    "                    self.ncounts[key[0]] = {}\n",
    "                    self.ncounts[key[0]][key[1]] = cnt\n",
    "\n",
    "                # 记录了: 对本项(key)，可能的下一项的(r , N_r)\n",
    "\n",
    "        return self.frequencies\n",
    "\n",
    "    # 输入gram,计算其对应插值参数d\n",
    "    # func 'd' not debugged yet\n",
    "    def d(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the interpolation coefficient.\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "        # r即本gram的出现频次\n",
    "        length = len(gram)\n",
    "        r = self.frequencies[length].get(gram, self.eps)\n",
    "\n",
    "        # 调用self.ncounts， 查看下一项的不同频次的出现次数\n",
    "        if gram not in self._d:\n",
    "            # print(gram)\n",
    "            # print(gram[:-1])\n",
    "            # TODO: calculates the value of $d'$\n",
    "            ncounts = self.ncounts[gram[:-1]]  # 查看前缀, counts是存储频次的字典\n",
    "            # print(ncounts)\n",
    "\n",
    "            def counts(num):\n",
    "                if num in ncounts:\n",
    "                    return float(ncounts[num])\n",
    "                else:\n",
    "                    return float(self.eps)  # 如有报错,改为0\n",
    "\n",
    "            lamda = float(counts(1) / (counts(1) - 8 * counts(8)))\n",
    "            N_r = counts(r)\n",
    "            N_r_plus1 = counts(r + 1)\n",
    "            d_dot = lamda * ((r + 1) * N_r_plus1) / (r * N_r) + 1 - lamda  # 求出各项参数 及d'\n",
    "\n",
    "            self._d[gram] = (d_dot, 1.0)\n",
    "\n",
    "        # self._d[gram] = (numerator1 / denominator, - numerator2 / denominator)  # 10/19 ques ?\n",
    "\n",
    "        if r > 7:\n",
    "            return self._d[gram][1]\n",
    "        else:\n",
    "            return self._d[gram][0]\n",
    "\n",
    "    def alpha(self, gram: Gram) -> float:  # '回退' 意为降低gram大小要求\n",
    "        \"\"\"\n",
    "        Calculates the back-off weight alpha(`gram`)\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)\n",
    "        # 概率存放在 disfrequencies 内\n",
    "        if gram not in self._alpha[n]:\n",
    "            if gram in self.frequencies[n - 1]:\n",
    "                # TODO: calculates the value of $\\alpha$\n",
    "\n",
    "                # V+, V- Accumulated, sum add\n",
    "                V_plus = []\n",
    "                V_minus = []\n",
    "\n",
    "                sum_pls = 0\n",
    "                sum_minus = 0\n",
    "                for i in range(1, self.vocab_size):\n",
    "                    # 添加元素:查询V+和V-\n",
    "                    index = len(gram)  # 减一是原长度 checked in test_funcs 10/19 21:35\n",
    "                    gram_add = list(gram)\n",
    "                    gram_add.append(i)\n",
    "                    gram_add = tuple(gram_add)\n",
    "\n",
    "                    if gram_add in self.frequencies[index]:\n",
    "                        # print(gram_add)\n",
    "                        # print(self.frequencies[index].get(gram_add, 0))\n",
    "                        V_plus.append(i)\n",
    "                        sum_pls = sum_pls + self.disfrequencies[index].get(gram_add, 0)  # checked in test\n",
    "                        # sum_pls = sum_pls + self.disfrequencies[index].get(gram_add, 0)\n",
    "                    else:\n",
    "                        V_minus.append(i)\n",
    "                        gram_minus = gram_add[1:]\n",
    "                        index = index - 1\n",
    "\n",
    "                        sum_minus = sum_minus + self.disfrequencies[index - 1].get(gram_minus, self.eps)\n",
    "\n",
    "                numerator = 1 - sum_pls\n",
    "                denominator = 1 - sum_minus\n",
    "\n",
    "                self._alpha[n][gram] = numerator / denominator\n",
    "            else:\n",
    "                self._alpha[n][gram] = 1.\n",
    "        return self._alpha[n][gram]\n",
    "\n",
    "    def __getitem__(self, gram: Gram) -> float:  # 计算回退概率(调用之前实现的函数)\n",
    "        \"\"\"\n",
    "        Calculates smoothed conditional probability P(`gram[-1]`|`gram[:-1]`).\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram) - 1\n",
    "\n",
    "        if gram not in self.disfrequencies[n]:\n",
    "            if n > 0:\n",
    "                # TODO: calculates the smoothed probability value according to the formulate\n",
    "\n",
    "                # 1:C>0, use param_d\n",
    "                if self.disfrequencies[n].get(gram, 0) > 0:\n",
    "                    param_d = self.d(gram)\n",
    "                    # print('param_d = ', param_d)\n",
    "                    Ck = self.frequencies[n][gram]\n",
    "                    Ck_1 = self.frequencies[n - 1][gram[:-1]]\n",
    "                    P = param_d * Ck / Ck_1\n",
    "                    self.disfrequencies[n][gram] = P\n",
    "\n",
    "                # 2:C=0, use param_a\n",
    "                else:\n",
    "                    param_a = self.alpha(gram[:-1])\n",
    "                    # print('param_a = ', param_a)\n",
    "                    # P = self.disfrequencies[n-1].get(gram[1:], self.eps) * param_a\n",
    "                    P = self.__getitem__(gram[1:]) * param_a\n",
    "                    self.disfrequencies[n][gram] = P\n",
    "\n",
    "            # 第一项\n",
    "            else:\n",
    "                self.disfrequencies[n][gram] = self.frequencies[n].get(gram, self.eps) / float(len(self.frequencies[0]))\n",
    "        return self.disfrequencies[n][gram]\n",
    "\n",
    "    # 交叉熵\n",
    "    def log_prob(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the log probability of the given sentence. Assumes that the\n",
    "        first token is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        log_prob = 0.\n",
    "        cnt = 0\n",
    "        n = self.n\n",
    "        for i in range(2, len(sentence) + 1):\n",
    "            # TODO: calculates the log probability\n",
    "            # 思路是递进地对每一个ngram求概率\n",
    "            cnt = float(cnt+1)\n",
    "            length = min(n, i)\n",
    "            gram = tuple(sentence[i-length:i])  # 可能超出范围,有待测试\n",
    "            # print(len(gram))\n",
    "            # print(gram)\n",
    "            log_prob = log_prob + math.log2(self.__getitem__(gram))\n",
    "\n",
    "        log_prob = - log_prob / cnt\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "    def ppl(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the PPL of the given sentence. Assumes that the first token\n",
    "        is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        # calculates the PPL\n",
    "\n",
    "        PPL = 1.\n",
    "        cnt = 0\n",
    "        n = self.n\n",
    "        for i in range(2, len(sentence) + 1):\n",
    "            # calculates the log probability\n",
    "            cnt = cnt + 1\n",
    "            length = min(n, i)\n",
    "            gram = tuple(sentence[i - length + 1:i])  # 可能超出范围,有待测试\n",
    "            PPL = PPL / self.__getitem__(gram)\n",
    "\n",
    "        # print(PPL)\n",
    "        # print(cnt)\n",
    "\n",
    "        PPL = math.pow(PPL, 1/cnt)\n",
    "        return PPL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbcf678",
   "metadata": {},
   "source": [
    "### 训练与测试\n",
    "\n",
    "现在数据与模型均已齐备，可以训练并测试了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd706656",
   "metadata": {},
   "source": [
    "训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db5f988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped model.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "model = NGramModel(len(vocabulary))\n",
    "model.learn(corpus)\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pkl.dump(vocabulary, f)\n",
    "    pkl.dump(model, f)\n",
    "\n",
    "print(\"Dumped model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f97ca6",
   "metadata": {},
   "source": [
    "在测试集上测试计算困惑度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf8b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.\n",
      "331.7920174085309\n",
      "5.563344879350672\n",
      "7.681535885346684\n",
      "9.231407600197334\n",
      "2058.5496313403114\n",
      "5.170079774739926\n",
      "7.346955771855707\n",
      "6.708097135516273\n",
      "35.8399602566888\n",
      "7.800398424769536\n",
      "4.98059473069868\n",
      "5.740536545302932\n",
      "106.72802995303684\n",
      "24.55205021534513\n",
      "10.23939484159557\n",
      "14.050691211160322\n",
      "22.110248446631623\n",
      "560.811966536818\n",
      "7.498264092662078\n",
      "12.986887242470827\n",
      "13.721430265811495\n",
      "31.509170294957258\n",
      "17.723829181289716\n",
      "16.66076665181356\n",
      "10.942950730870209\n",
      "5.626950182695882\n",
      "11.97790166750024\n",
      "4.039504825190963\n",
      "10.395047853265448\n",
      "6.724573289134925\n",
      "14.531174264417785\n",
      "24.018847141286066\n",
      "9.833317186292778\n",
      "4.5076702318899775\n",
      "11.171390631558685\n",
      "10.245512255962051\n",
      "14.991131855280008\n",
      "16.742101922318962\n",
      "9.203150354256293\n",
      "30.42616456485172\n",
      "5.835219219013738\n",
      "31.767545278188052\n",
      "9.49133763841313\n",
      "6.593217047976487\n",
      "11.942381153275067\n",
      "54.20236759260752\n",
      "14.461080582736281\n",
      "31.70004137384701\n",
      "47.85187170003562\n",
      "13.874095968074558\n",
      "Avg:  74.96187670395679\n"
     ]
    }
   ],
   "source": [
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    vocabulary = pkl.load(f)\n",
    "    model = pkl.load(f)\n",
    "print(\"Loaded model.\")\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.test\") as f:\n",
    "    test_set = list(map(lambda l: l.strip(), f.readlines()))\n",
    "test_corpus = normaltokenize(test_set)\n",
    "test_corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            test_corpus))\n",
    "ppls = []\n",
    "for t in test_corpus:\n",
    "    ppls.append(model.ppl(t))\n",
    "    print(ppls[-1])\n",
    "print(\"Avg: \", sum(ppls)/len(ppls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c35fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
