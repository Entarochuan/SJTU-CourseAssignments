{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95c2060",
   "metadata": {},
   "source": [
    "#### Part1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9144, 200, 200, 3)\n",
      "(9144,)\n",
      "6400 train samples\n",
      "2744 test samples\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from keras import backend as K\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "IMAGE_SIZE = 200\n",
    "\n",
    "\n",
    "# 按照指定图像大小调整尺寸\n",
    "def resize_image(image, height=IMAGE_SIZE, width=IMAGE_SIZE):\n",
    "    return cv2.resize(image, (height, width))\n",
    "\n",
    "\n",
    "def read_path(path_name):\n",
    "    for dir_item in os.listdir(path_name):\n",
    "        full_path = os.path.abspath(os.path.join(path_name, dir_item))\n",
    "\n",
    "        if os.path.isdir(full_path):  # 如果是文件夹，继续递归调用\n",
    "            read_path(full_path)\n",
    "        else:  # 文件\n",
    "            if dir_item.endswith('.jpg') or dir_item.endswith('.JPG') or dir_item.endswith('.png'):\n",
    "                image = cv2.imread(full_path)\n",
    "                image = resize_image(image)\n",
    "                images.append(image)\n",
    "                labels.append(path_name)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_dataset(path_name):\n",
    "    images, labels = read_path(path_name)\n",
    "\n",
    "    images = np.array(images)\n",
    "    print(images.shape)\n",
    "    category = []\n",
    "    for i in labels:\n",
    "        category.append(i.split('/')[-1])\n",
    "    temp = list(set(category))\n",
    "    dic = {}\n",
    "    for i in range(len(temp)):\n",
    "        dic[temp[i]] = i\n",
    "    for i in range(len(category)):\n",
    "        labels[i] = dic[category[i]]\n",
    "    labels = np.array(labels)\n",
    "    print(labels.shape)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, path_name):\n",
    "        # 训练集\n",
    "        self.train_images = None\n",
    "        self.train_lb = None\n",
    "\n",
    "        # 测试集\n",
    "        self.test_images = None\n",
    "        self.test_lb = None\n",
    "\n",
    "        # 数据集加载路径\n",
    "        self.path_name = path_name\n",
    "\n",
    "        # 当前库采用的维度顺序\n",
    "        self.input_shape = None\n",
    "\n",
    "    # 加载数据集并按照交叉验证的原则划分数据集并进行相关预处理工作\n",
    "    def load(self, img_rows=IMAGE_SIZE, img_cols=IMAGE_SIZE,\n",
    "             img_channels=3, nb_classes=102):\n",
    "        # 加载数据集到内存\n",
    "        images, labels = load_dataset(self.path_name)\n",
    "\n",
    "        train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.3,\n",
    "                                                                                random_state=random.randint(0, 100))\n",
    "\n",
    "        # 当前的维度顺序如果为'th'，则输入图片数据时的顺序为：channels,rows,cols，否则:rows,cols,channels\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            train_images = train_images.reshape(train_images.shape[0], img_channels, img_rows, img_cols)\n",
    "            test_images = test_images.reshape(test_images.shape[0], img_channels, img_rows, img_cols)\n",
    "            self.input_shape = (img_channels, img_rows, img_cols)\n",
    "        else:\n",
    "            train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            self.input_shape = (img_rows, img_cols, img_channels)\n",
    "\n",
    "            # 输出训练集、验证集、测试集的数量\n",
    "            print(train_images.shape[0], 'train samples')\n",
    "            print(test_images.shape[0], 'test samples')\n",
    "\n",
    "            self.train_lb = train_labels\n",
    "            self.test_lb = test_labels\n",
    "\n",
    "            # 像素数据浮点化以便归一化\n",
    "            train_images = train_images.astype('float32')\n",
    "            test_images = test_images.astype('float32')\n",
    "\n",
    "            # 将其归一化,图像的各像素值归一化到0~1区间\n",
    "            train_images /= 255\n",
    "            test_images /= 255\n",
    "\n",
    "            self.train_images = train_images\n",
    "            self.test_images = test_images\n",
    "\n",
    "\n",
    "data = Dataset('./kaggle/caltech-101/')\n",
    "\n",
    "data.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a02cb",
   "metadata": {},
   "source": [
    "#### Part2 TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554dd133",
   "metadata": {},
   "source": [
    "TODO 利用SIFT从训练图像中提取特征  \n",
    "\n",
    "如果有需要，你也可以在pass之外的地方填写相关代码，请自便，下同。\n",
    "\n",
    "vec_dict 第i项： i为类别，对应的字典为所有属于该类的sift特征点的信息。注意：kp与des一一对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "169be5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dict = {i:{'kp':[], 'des':[]} for i in range(102)}\n",
    "# des 为128维度的描述向量\n",
    "# 102 是数据集的类别个数...QAQ\n",
    "sift = cv2.SIFT_create()\n",
    "# print(data.train_images.shape[0])  # 6400\n",
    "for i in range(data.train_images.shape[0]):\n",
    "    # if i==10:\n",
    "    #     break\n",
    "    tep = cv2.normalize(data.train_images[i], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    kp_vector, des_vector = sift.detectAndCompute(tep, None)\n",
    "    class_num = data.train_lb[i]\n",
    "    \n",
    "    kp_vector = list(kp_vector)\n",
    "    des_vector = list(des_vector)\n",
    "    vec_dict[class_num]['kp'].extend(kp_vector)\n",
    "    # print(len(vec_dict[class_num]['kp']))\n",
    "    vec_dict[class_num]['des'].extend(des_vector)\n",
    "    # print(len(vec_dict[class_num]['kp']))\n",
    "    \n",
    "    # print(len(des_vector[0]))\n",
    "\n",
    "# print(vec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f640f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< cv2.KeyPoint 00000234AFEBD620>\n",
      "[ 29.   0.   0.   0.   0.   0.   0.   0. 204.   7.   0.   0.   0.   0.\n",
      "   0.  11. 137.   7.   0.   0.   0.   0.   1.  35.  33.  11.  12.  10.\n",
      "   2.   0.   0.   7.  39.   0.   0.   0.   0.   0.   0.   1. 204.   5.\n",
      "   0.   0.   0.   0.   0.  46. 134.   6.   5.   6.   1.   1.   7.  66.\n",
      "   5.   5.  14.   8.   2.   4.   3.   4.  39.   0.   0.   0.   0.   0.\n",
      "   0.   0. 204.  41.   1.   0.   0.   0.   0.   6. 132.  39.  16.  13.\n",
      "   1.   0.   3.  14.   2.   7.  16.   4.   0.   0.  24.  20.  29.   0.\n",
      "   0.   0.   0.   0.   0.   0. 204.   8.   0.   0.   0.   0.   0.  19.\n",
      " 124.   7.   1.   0.   0.   0.   3.  67.   9.   1.   2.   2.   1.   1.\n",
      "  24.  36.]\n"
     ]
    }
   ],
   "source": [
    "print(vec_dict[0]['kp'][0])\n",
    "print(vec_dict[0]['des'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f71f9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "bneck_value = float(\"inf\")\n",
    "for i in range(102):\n",
    "    if len(vec_dict[i]['kp']) < bneck_value:\n",
    "        bneck_value = len(vec_dict[i]['kp'])\n",
    "        # print(bneck_value)\n",
    "  \n",
    "for i in range(102):\n",
    "    kp_list = vec_dict[i]['kp'] = sorted((vec_dict[i]['kp']),\n",
    "                                         key=lambda x: x.response,\n",
    "                                         reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce6a9c",
   "metadata": {},
   "source": [
    "TODO 为每个类别选择同样多的特征点用于聚类。特征点个数bneck_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0721b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324258, 128)\n"
     ]
    }
   ],
   "source": [
    "vec_list = vec_dict[0]['des'][0:bneck_value]\n",
    "# print(bneck_value) 3379\n",
    "for i in range(1, 102):\n",
    "    vec_list = vec_list + vec_dict[i]['des'][0:bneck_value]\n",
    "vec_list = np.float64(vec_list)\n",
    "print(vec_list.shape) #(344658, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ec41d",
   "metadata": {},
   "source": [
    "TODO 对提取出的特征点使用Kmeans聚类，设定合适的聚类中心个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a721de84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#####\n",
    "N_clusters = 50 # just 4 try\n",
    "kmeans = KMeans(n_clusters=N_clusters)\n",
    "kmeans.fit(vec_list)\n",
    "#####\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc84c43",
   "metadata": {},
   "source": [
    "TODO 利用直方图统计每张图像中的特征点所属聚类中心的个数，将直方图归一化后便得到图像的特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43aa1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = data.train_images.shape[0]\n",
    "hist_vector = np.zeros((num_images, N_clusters))\n",
    "for i in range(num_images):\n",
    "    # if i == 2:\n",
    "    #     break\n",
    "    tep = cv2.normalize(data.train_images[i], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    kp_vector, des_vector = sift.detectAndCompute(tep, None)\n",
    "    # print(des_vector.shape)\n",
    "    des_vector = np.array(des_vector, dtype=np.float64)\n",
    "    label_test = kmeans.predict(des_vector)\n",
    "    # print(label_test)\n",
    "    for label in label_test:\n",
    "        hist_vector[i][label] = hist_vector[i][label] + 1\n",
    "    # for si in des_vector:\n",
    "    #     si = np.array(si.reshape(1, -1), dtype=np.float64)\n",
    "    #     # print(si.shape)\n",
    "    #     si_label = kmeans.predict(si)\n",
    "    #     # print(si_label)\n",
    "    #     hist_vector[i][si_label] = hist_vector[i][si_label] + 1\n",
    "    \n",
    "    # dst = np.zeros_like(hist_vector[i])\n",
    "    # print(hist_vector)\n",
    "\n",
    "    cv2.normalize(hist_vector[i], hist_vector[i], 0, 1, cv2.NORM_MINMAX) # 归一化\n",
    "    # print(hist_vector[i])\n",
    "# print(hist_vector[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dd5e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0212766  1.         0.27659574 0.04255319 0.21276596 0.27659574\n",
      " 0.0212766  0.10638298 0.08510638 0.29787234 0.08510638 0.17021277\n",
      " 0.12765957 0.0212766  0.08510638 0.46808511 0.04255319 0.\n",
      " 0.10638298 0.08510638 0.06382979 0.10638298 0.17021277 0.25531915\n",
      " 0.0212766  0.04255319 0.36170213 0.04255319 0.12765957 0.19148936\n",
      " 0.06382979 0.38297872 0.04255319 0.57446809 0.12765957 0.12765957\n",
      " 0.         0.10638298 0.38297872 0.17021277 0.12765957 0.\n",
      " 0.65957447 0.17021277 0.10638298 0.10638298 0.25531915 0.04255319\n",
      " 0.04255319 0.12765957]\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# print(data.train_lb[100])\n",
    "print(hist_vector[0])\n",
    "print(data.train_lb[6399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c812206a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier SVM\n",
    "from sklearn import svm\n",
    "classifier = svm.SVC(probability=True)\n",
    "classifier.fit(hist_vector, data.train_lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace4f08",
   "metadata": {},
   "source": [
    "TODO 构建测试集并计算模型准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca074621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM: 0.33090379008746357\n"
     ]
    }
   ],
   "source": [
    "# 使用SVM构建分类器\n",
    "# 你可以自行构建分类器，也可以使用SVM\n",
    "# TODO 构建测试集并计算模型准确率\n",
    "num_test_images = data.test_images.shape[0]\n",
    "hist_test_vector = np.zeros((num_test_images, N_clusters))\n",
    "# print(num_test_images)\n",
    "for i in range(num_test_images):\n",
    "    # if i==2:\n",
    "    #     break\n",
    "    tep = cv2.normalize(data.test_images[i], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    kp_vector, des_vector = sift.detectAndCompute(tep, None)\n",
    "    des_vector = np.array(des_vector, dtype=np.float64)\n",
    "    \n",
    "    label_pred_test = kmeans.predict(des_vector)\n",
    "    # label_pred_test = classifier.predict(vec_test)\n",
    "    # print()\n",
    "    for label in label_pred_test:\n",
    "        hist_test_vector[i][label] = hist_test_vector[i][label] + 1\n",
    "        \n",
    "    cv2.normalize(hist_test_vector[i], hist_test_vector[i], 0, 1, cv2.NORM_MINMAX)\n",
    "    # print(hist_test_vector[i])\n",
    "    # print()\n",
    "\n",
    "\n",
    "acc = classifier.predict(hist_test_vector)-data.test_lb\n",
    "tep = len(acc[acc==0])\n",
    "# print(tep)\n",
    "# print(len(data.test_lb))\n",
    "print('Accuracy of SVM:', tep/len(data.test_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cb59637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels   = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        feature = self.features[i]\n",
    "        label   = self.labels[i]\n",
    "        \n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "704cbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "train_set = My_Dataset(hist_vector, data.train_lb)\n",
    "test_set  = My_Dataset(hist_test_vector, data.test_lb)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16)\n",
    "test_loader  = DataLoader(test_set, batch_size=16)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=50, out_features=256), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(256, 1024), \n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(0.2), \n",
    "    nn.Linear(1024, 102),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.8, weight_decay=1e-5)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4cf13a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YichuanMa\\AppData\\Local\\Temp\\ipykernel_29692\\4259749347.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "C:\\Users\\YichuanMa\\AppData\\Local\\Temp\\ipykernel_29692\\4259749347.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch:100, loss:4.624363899230957\n",
      "epoch: 1, batch:200, loss:4.624178886413574\n",
      "epoch: 1, batch:300, loss:4.623955726623535\n",
      "epoch: 1, batch:400, loss:4.623724937438965\n",
      "epoch: 2, batch:100, loss:4.6234822273254395\n",
      "epoch: 2, batch:200, loss:4.623290061950684\n",
      "epoch: 2, batch:300, loss:4.623076438903809\n",
      "epoch: 2, batch:400, loss:4.622590065002441\n",
      "epoch: 3, batch:100, loss:4.6223955154418945\n",
      "epoch: 3, batch:200, loss:4.622158527374268\n",
      "epoch: 3, batch:300, loss:4.621729850769043\n",
      "epoch: 3, batch:400, loss:4.621478080749512\n",
      "epoch: 4, batch:100, loss:4.621064186096191\n",
      "epoch: 4, batch:200, loss:4.620717525482178\n",
      "epoch: 4, batch:300, loss:4.620311737060547\n",
      "epoch: 4, batch:400, loss:4.619791030883789\n",
      "epoch: 5, batch:100, loss:4.6192450523376465\n",
      "epoch: 5, batch:200, loss:4.619004726409912\n",
      "epoch: 5, batch:300, loss:4.618350028991699\n",
      "epoch: 5, batch:400, loss:4.617878437042236\n",
      "epoch: 6, batch:100, loss:4.617140769958496\n",
      "epoch: 6, batch:200, loss:4.616617202758789\n",
      "epoch: 6, batch:300, loss:4.615696430206299\n",
      "epoch: 6, batch:400, loss:4.615077972412109\n",
      "epoch: 7, batch:100, loss:4.614111423492432\n",
      "epoch: 7, batch:200, loss:4.612536907196045\n",
      "epoch: 7, batch:300, loss:4.611496448516846\n",
      "epoch: 7, batch:400, loss:4.609996318817139\n",
      "epoch: 8, batch:100, loss:4.608896255493164\n",
      "epoch: 8, batch:200, loss:4.607052326202393\n",
      "epoch: 8, batch:300, loss:4.604226112365723\n",
      "epoch: 8, batch:400, loss:4.602062225341797\n",
      "epoch: 9, batch:100, loss:4.5985002517700195\n",
      "epoch: 9, batch:200, loss:4.59334659576416\n",
      "epoch: 9, batch:300, loss:4.587640762329102\n",
      "epoch: 9, batch:400, loss:4.579396724700928\n",
      "epoch: 10, batch:100, loss:4.564789295196533\n",
      "epoch: 10, batch:200, loss:4.533931732177734\n",
      "epoch: 10, batch:300, loss:4.450575828552246\n",
      "epoch: 10, batch:400, loss:4.008289813995361\n",
      "epoch: 11, batch:100, loss:3.6986238956451416\n",
      "epoch: 11, batch:200, loss:3.665658473968506\n",
      "epoch: 11, batch:300, loss:3.660299062728882\n",
      "epoch: 11, batch:400, loss:3.6548044681549072\n",
      "epoch: 12, batch:100, loss:3.65114688873291\n",
      "epoch: 12, batch:200, loss:3.6486904621124268\n",
      "epoch: 12, batch:300, loss:3.647369861602783\n",
      "epoch: 12, batch:400, loss:3.646469831466675\n",
      "epoch: 13, batch:100, loss:3.645850419998169\n",
      "epoch: 13, batch:200, loss:3.6453237533569336\n",
      "epoch: 13, batch:300, loss:3.6450157165527344\n",
      "epoch: 13, batch:400, loss:3.6445577144622803\n",
      "epoch: 14, batch:100, loss:3.6445815563201904\n",
      "epoch: 14, batch:200, loss:3.6440889835357666\n",
      "epoch: 14, batch:300, loss:3.6440269947052\n",
      "epoch: 14, batch:400, loss:3.643737554550171\n",
      "epoch: 15, batch:100, loss:3.6435956954956055\n",
      "epoch: 15, batch:200, loss:3.6438443660736084\n",
      "epoch: 15, batch:300, loss:3.6432688236236572\n",
      "epoch: 15, batch:400, loss:3.643033981323242\n",
      "epoch: 16, batch:100, loss:3.6432759761810303\n",
      "epoch: 16, batch:200, loss:3.6432957649230957\n",
      "epoch: 16, batch:300, loss:3.642845392227173\n",
      "epoch: 16, batch:400, loss:3.6428794860839844\n",
      "epoch: 17, batch:100, loss:3.642942428588867\n",
      "epoch: 17, batch:200, loss:3.642793893814087\n",
      "epoch: 17, batch:300, loss:3.642674446105957\n",
      "epoch: 17, batch:400, loss:3.642815113067627\n",
      "epoch: 18, batch:100, loss:3.642706871032715\n",
      "epoch: 18, batch:200, loss:3.6425979137420654\n",
      "epoch: 18, batch:300, loss:3.6428070068359375\n",
      "epoch: 18, batch:400, loss:3.6425797939300537\n",
      "epoch: 19, batch:100, loss:3.6426568031311035\n",
      "epoch: 19, batch:200, loss:3.642496109008789\n",
      "epoch: 19, batch:300, loss:3.642606735229492\n",
      "epoch: 19, batch:400, loss:3.642552375793457\n",
      "epoch: 20, batch:100, loss:3.6425046920776367\n",
      "epoch: 20, batch:200, loss:3.6425156593322754\n",
      "epoch: 20, batch:300, loss:3.642418384552002\n",
      "epoch: 20, batch:400, loss:3.6424341201782227\n",
      "epoch: 21, batch:100, loss:3.642362594604492\n",
      "epoch: 21, batch:200, loss:3.6424410343170166\n",
      "epoch: 21, batch:300, loss:3.6423697471618652\n",
      "epoch: 21, batch:400, loss:3.6423590183258057\n",
      "epoch: 22, batch:100, loss:3.6423118114471436\n",
      "epoch: 22, batch:200, loss:3.6423001289367676\n",
      "epoch: 22, batch:300, loss:3.642240524291992\n",
      "epoch: 22, batch:400, loss:3.642362117767334\n",
      "epoch: 23, batch:100, loss:3.6422019004821777\n",
      "epoch: 23, batch:200, loss:3.642341136932373\n",
      "epoch: 23, batch:300, loss:3.6422131061553955\n",
      "epoch: 23, batch:400, loss:3.6423635482788086\n",
      "epoch: 24, batch:100, loss:3.6422152519226074\n",
      "epoch: 24, batch:200, loss:3.642341375350952\n",
      "epoch: 24, batch:300, loss:3.64211106300354\n",
      "epoch: 24, batch:400, loss:3.6421687602996826\n",
      "epoch: 25, batch:100, loss:3.6420950889587402\n",
      "epoch: 25, batch:200, loss:3.642134189605713\n",
      "epoch: 25, batch:300, loss:3.6421210765838623\n",
      "epoch: 25, batch:400, loss:3.6422066688537598\n",
      "epoch: 26, batch:100, loss:3.6421048641204834\n",
      "epoch: 26, batch:200, loss:3.64208984375\n",
      "epoch: 26, batch:300, loss:3.642101526260376\n",
      "epoch: 26, batch:400, loss:3.6420905590057373\n",
      "epoch: 27, batch:100, loss:3.642089366912842\n",
      "epoch: 27, batch:200, loss:3.642073631286621\n",
      "epoch: 27, batch:300, loss:3.642012119293213\n",
      "epoch: 27, batch:400, loss:3.6421446800231934\n",
      "epoch: 28, batch:100, loss:3.642115831375122\n",
      "epoch: 28, batch:200, loss:3.642080545425415\n",
      "epoch: 28, batch:300, loss:3.642040729522705\n",
      "epoch: 28, batch:400, loss:3.642080307006836\n",
      "epoch: 29, batch:100, loss:3.6420977115631104\n",
      "epoch: 29, batch:200, loss:3.642042636871338\n",
      "epoch: 29, batch:300, loss:3.6421010494232178\n",
      "epoch: 29, batch:400, loss:3.6420035362243652\n",
      "epoch: 30, batch:100, loss:3.642030954360962\n",
      "epoch: 30, batch:200, loss:3.641998052597046\n",
      "epoch: 30, batch:300, loss:3.642092704772949\n",
      "epoch: 30, batch:400, loss:3.6420481204986572\n",
      "epoch: 31, batch:100, loss:3.6420276165008545\n",
      "epoch: 31, batch:200, loss:3.6419622898101807\n",
      "epoch: 31, batch:300, loss:3.6420459747314453\n",
      "epoch: 31, batch:400, loss:3.6419308185577393\n",
      "epoch: 32, batch:100, loss:3.642009735107422\n",
      "epoch: 32, batch:200, loss:3.641951322555542\n",
      "epoch: 32, batch:300, loss:3.641970157623291\n",
      "epoch: 32, batch:400, loss:3.6419553756713867\n",
      "epoch: 33, batch:100, loss:3.6419739723205566\n",
      "epoch: 33, batch:200, loss:3.642028331756592\n",
      "epoch: 33, batch:300, loss:3.6420321464538574\n",
      "epoch: 33, batch:400, loss:3.6419570446014404\n",
      "epoch: 34, batch:100, loss:3.6419713497161865\n",
      "epoch: 34, batch:200, loss:3.6419434547424316\n",
      "epoch: 34, batch:300, loss:3.6419897079467773\n",
      "epoch: 34, batch:400, loss:3.6419429779052734\n",
      "epoch: 35, batch:100, loss:3.641906261444092\n",
      "epoch: 35, batch:200, loss:3.641920804977417\n",
      "epoch: 35, batch:300, loss:3.6419692039489746\n",
      "epoch: 35, batch:400, loss:3.6419568061828613\n",
      "epoch: 36, batch:100, loss:3.6418843269348145\n",
      "epoch: 36, batch:200, loss:3.6419107913970947\n",
      "epoch: 36, batch:300, loss:3.6419787406921387\n",
      "epoch: 36, batch:400, loss:3.6418769359588623\n",
      "epoch: 37, batch:100, loss:3.6419408321380615\n",
      "epoch: 37, batch:200, loss:3.641897201538086\n",
      "epoch: 37, batch:300, loss:3.6419012546539307\n",
      "epoch: 37, batch:400, loss:3.6419076919555664\n",
      "epoch: 38, batch:100, loss:3.6418840885162354\n",
      "epoch: 38, batch:200, loss:3.6419026851654053\n",
      "epoch: 38, batch:300, loss:3.6419408321380615\n",
      "epoch: 38, batch:400, loss:3.6418986320495605\n",
      "epoch: 39, batch:100, loss:3.6418943405151367\n",
      "epoch: 39, batch:200, loss:3.6418797969818115\n",
      "epoch: 39, batch:300, loss:3.6419034004211426\n",
      "epoch: 39, batch:400, loss:3.6419613361358643\n",
      "epoch: 40, batch:100, loss:3.6418614387512207\n",
      "epoch: 40, batch:200, loss:3.6418838500976562\n",
      "epoch: 40, batch:300, loss:3.6419272422790527\n",
      "epoch: 40, batch:400, loss:3.641880989074707\n",
      "epoch: 41, batch:100, loss:3.6418583393096924\n",
      "epoch: 41, batch:200, loss:3.641860246658325\n",
      "epoch: 41, batch:300, loss:3.641866683959961\n",
      "epoch: 41, batch:400, loss:3.641860246658325\n",
      "epoch: 42, batch:100, loss:3.641874313354492\n",
      "epoch: 42, batch:200, loss:3.641893148422241\n",
      "epoch: 42, batch:300, loss:3.6418802738189697\n",
      "epoch: 42, batch:400, loss:3.6418776512145996\n",
      "epoch: 43, batch:100, loss:3.6418914794921875\n",
      "epoch: 43, batch:200, loss:3.641904592514038\n",
      "epoch: 43, batch:300, loss:3.64190673828125\n",
      "epoch: 43, batch:400, loss:3.641847610473633\n",
      "epoch: 44, batch:100, loss:3.6418795585632324\n",
      "epoch: 44, batch:200, loss:3.6418607234954834\n",
      "epoch: 44, batch:300, loss:3.641897439956665\n",
      "epoch: 44, batch:400, loss:3.6418633460998535\n",
      "epoch: 45, batch:100, loss:3.6418561935424805\n",
      "epoch: 45, batch:200, loss:3.641822099685669\n",
      "epoch: 45, batch:300, loss:3.6418182849884033\n",
      "epoch: 45, batch:400, loss:3.64184308052063\n",
      "epoch: 46, batch:100, loss:3.6418302059173584\n",
      "epoch: 46, batch:200, loss:3.641845941543579\n",
      "epoch: 46, batch:300, loss:3.6418888568878174\n",
      "epoch: 46, batch:400, loss:3.641855001449585\n",
      "epoch: 47, batch:100, loss:3.6418569087982178\n",
      "epoch: 47, batch:200, loss:3.641848564147949\n",
      "epoch: 47, batch:300, loss:3.6418559551239014\n",
      "epoch: 47, batch:400, loss:3.6418910026550293\n",
      "epoch: 48, batch:100, loss:3.6418302059173584\n",
      "epoch: 48, batch:200, loss:3.6418206691741943\n",
      "epoch: 48, batch:300, loss:3.6418533325195312\n",
      "epoch: 48, batch:400, loss:3.64182710647583\n",
      "epoch: 49, batch:100, loss:3.6418421268463135\n",
      "epoch: 49, batch:200, loss:3.641829252243042\n",
      "epoch: 49, batch:300, loss:3.641812562942505\n",
      "epoch: 49, batch:400, loss:3.6419482231140137\n",
      "epoch: 50, batch:100, loss:3.641833782196045\n",
      "epoch: 50, batch:200, loss:3.641842842102051\n",
      "epoch: 50, batch:300, loss:3.6418325901031494\n",
      "epoch: 50, batch:400, loss:3.641817092895508\n",
      "epoch: 51, batch:100, loss:3.6418135166168213\n",
      "epoch: 51, batch:200, loss:3.641815185546875\n",
      "epoch: 51, batch:300, loss:3.641847848892212\n",
      "epoch: 51, batch:400, loss:3.641803503036499\n",
      "epoch: 52, batch:100, loss:3.641799211502075\n",
      "epoch: 52, batch:200, loss:3.641805648803711\n",
      "epoch: 52, batch:300, loss:3.6418373584747314\n",
      "epoch: 52, batch:400, loss:3.6418490409851074\n",
      "epoch: 53, batch:100, loss:3.6418073177337646\n",
      "epoch: 53, batch:200, loss:3.641798973083496\n",
      "epoch: 53, batch:300, loss:3.6418304443359375\n",
      "epoch: 53, batch:400, loss:3.641836166381836\n",
      "epoch: 54, batch:100, loss:3.641822576522827\n",
      "epoch: 54, batch:200, loss:3.6418349742889404\n",
      "epoch: 54, batch:300, loss:3.641810894012451\n",
      "epoch: 54, batch:400, loss:3.6417970657348633\n",
      "epoch: 55, batch:100, loss:3.641814708709717\n",
      "epoch: 55, batch:200, loss:3.641817092895508\n",
      "epoch: 55, batch:300, loss:3.6418068408966064\n",
      "epoch: 55, batch:400, loss:3.6418566703796387\n",
      "epoch: 56, batch:100, loss:3.6417832374572754\n",
      "epoch: 56, batch:200, loss:3.6417860984802246\n",
      "epoch: 56, batch:300, loss:3.641805648803711\n",
      "epoch: 56, batch:400, loss:3.6418280601501465\n",
      "epoch: 57, batch:100, loss:3.641798973083496\n",
      "epoch: 57, batch:200, loss:3.641792058944702\n",
      "epoch: 57, batch:300, loss:3.641836166381836\n",
      "epoch: 57, batch:400, loss:3.641796827316284\n",
      "epoch: 58, batch:100, loss:3.6417884826660156\n",
      "epoch: 58, batch:200, loss:3.6418066024780273\n",
      "epoch: 58, batch:300, loss:3.641803741455078\n",
      "epoch: 58, batch:400, loss:3.6418042182922363\n",
      "epoch: 59, batch:100, loss:3.641799211502075\n",
      "epoch: 59, batch:200, loss:3.641800880432129\n",
      "epoch: 59, batch:300, loss:3.641800880432129\n",
      "epoch: 59, batch:400, loss:3.6418044567108154\n",
      "epoch: 60, batch:100, loss:3.641794204711914\n",
      "epoch: 60, batch:200, loss:3.6418075561523438\n",
      "epoch: 60, batch:300, loss:3.641814947128296\n",
      "epoch: 60, batch:400, loss:3.6417973041534424\n",
      "epoch: 61, batch:100, loss:3.641831874847412\n",
      "epoch: 61, batch:200, loss:3.641784429550171\n",
      "epoch: 61, batch:300, loss:3.6418094635009766\n",
      "epoch: 61, batch:400, loss:3.6418161392211914\n",
      "epoch: 62, batch:100, loss:3.6418075561523438\n",
      "epoch: 62, batch:200, loss:3.641798973083496\n",
      "epoch: 62, batch:300, loss:3.6418309211730957\n",
      "epoch: 62, batch:400, loss:3.6417860984802246\n",
      "epoch: 63, batch:100, loss:3.641793966293335\n",
      "epoch: 63, batch:200, loss:3.6417860984802246\n",
      "epoch: 63, batch:300, loss:3.6418118476867676\n",
      "epoch: 63, batch:400, loss:3.6417834758758545\n",
      "epoch: 64, batch:100, loss:3.641810655593872\n",
      "epoch: 64, batch:200, loss:3.6418111324310303\n",
      "epoch: 64, batch:300, loss:3.64180326461792\n",
      "epoch: 64, batch:400, loss:3.64176344871521\n",
      "epoch: 65, batch:100, loss:3.641794443130493\n",
      "epoch: 65, batch:200, loss:3.6417856216430664\n",
      "epoch: 65, batch:300, loss:3.641757011413574\n",
      "epoch: 65, batch:400, loss:3.6417789459228516\n",
      "epoch: 66, batch:100, loss:3.641759157180786\n",
      "epoch: 66, batch:200, loss:3.6417760848999023\n",
      "epoch: 66, batch:300, loss:3.6418018341064453\n",
      "epoch: 66, batch:400, loss:3.6417856216430664\n",
      "epoch: 67, batch:100, loss:3.6418049335479736\n",
      "epoch: 67, batch:200, loss:3.641798973083496\n",
      "epoch: 67, batch:300, loss:3.6417641639709473\n",
      "epoch: 67, batch:400, loss:3.641800880432129\n",
      "epoch: 68, batch:100, loss:3.641792058944702\n",
      "epoch: 68, batch:200, loss:3.6417582035064697\n",
      "epoch: 68, batch:300, loss:3.641784191131592\n",
      "epoch: 68, batch:400, loss:3.6417806148529053\n",
      "epoch: 69, batch:100, loss:3.6417925357818604\n",
      "epoch: 69, batch:200, loss:3.6417770385742188\n",
      "epoch: 69, batch:300, loss:3.6417899131774902\n",
      "epoch: 69, batch:400, loss:3.6417784690856934\n",
      "epoch: 70, batch:100, loss:3.641771078109741\n",
      "epoch: 70, batch:200, loss:3.641796588897705\n",
      "epoch: 70, batch:300, loss:3.6417720317840576\n",
      "epoch: 70, batch:400, loss:3.641784906387329\n",
      "epoch: 71, batch:100, loss:3.6417534351348877\n",
      "epoch: 71, batch:200, loss:3.6417465209960938\n",
      "epoch: 71, batch:300, loss:3.641754627227783\n",
      "epoch: 71, batch:400, loss:3.6417620182037354\n",
      "epoch: 72, batch:100, loss:3.6417479515075684\n",
      "epoch: 72, batch:200, loss:3.64176082611084\n",
      "epoch: 72, batch:300, loss:3.6417672634124756\n",
      "epoch: 72, batch:400, loss:3.641772508621216\n",
      "epoch: 73, batch:100, loss:3.6417741775512695\n",
      "epoch: 73, batch:200, loss:3.64174485206604\n",
      "epoch: 73, batch:300, loss:3.6418042182922363\n",
      "epoch: 73, batch:400, loss:3.641782522201538\n",
      "epoch: 74, batch:100, loss:3.641758918762207\n",
      "epoch: 74, batch:200, loss:3.6417951583862305\n",
      "epoch: 74, batch:300, loss:3.6417858600616455\n",
      "epoch: 74, batch:400, loss:3.641767978668213\n",
      "epoch: 75, batch:100, loss:3.6417624950408936\n",
      "epoch: 75, batch:200, loss:3.6417739391326904\n",
      "epoch: 75, batch:300, loss:3.6417436599731445\n",
      "epoch: 75, batch:400, loss:3.6417651176452637\n",
      "epoch: 76, batch:100, loss:3.6417601108551025\n",
      "epoch: 76, batch:200, loss:3.6417486667633057\n",
      "epoch: 76, batch:300, loss:3.641758918762207\n",
      "epoch: 76, batch:400, loss:3.641779661178589\n",
      "epoch: 77, batch:100, loss:3.641768455505371\n",
      "epoch: 77, batch:200, loss:3.641777276992798\n",
      "epoch: 77, batch:300, loss:3.64176082611084\n",
      "epoch: 77, batch:400, loss:3.641765594482422\n",
      "epoch: 78, batch:100, loss:3.6417620182037354\n",
      "epoch: 78, batch:200, loss:3.6417603492736816\n",
      "epoch: 78, batch:300, loss:3.64176344871521\n",
      "epoch: 78, batch:400, loss:3.6417598724365234\n",
      "epoch: 79, batch:100, loss:3.6417465209960938\n",
      "epoch: 79, batch:200, loss:3.6417782306671143\n",
      "epoch: 79, batch:300, loss:3.6417593955993652\n",
      "epoch: 79, batch:400, loss:3.6417763233184814\n",
      "epoch: 80, batch:100, loss:3.641753911972046\n",
      "epoch: 80, batch:200, loss:3.6417741775512695\n",
      "epoch: 80, batch:300, loss:3.6417617797851562\n",
      "epoch: 80, batch:400, loss:3.6417624950408936\n",
      "epoch: 81, batch:100, loss:3.641763925552368\n",
      "epoch: 81, batch:200, loss:3.6417572498321533\n",
      "epoch: 81, batch:300, loss:3.6417500972747803\n",
      "epoch: 81, batch:400, loss:3.641747236251831\n",
      "epoch: 82, batch:100, loss:3.641770839691162\n",
      "epoch: 82, batch:200, loss:3.641761064529419\n",
      "epoch: 82, batch:300, loss:3.6417784690856934\n",
      "epoch: 82, batch:400, loss:3.641763210296631\n",
      "epoch: 83, batch:100, loss:3.641765832901001\n",
      "epoch: 83, batch:200, loss:3.641754627227783\n",
      "epoch: 83, batch:300, loss:3.6417579650878906\n",
      "epoch: 83, batch:400, loss:3.6417734622955322\n",
      "epoch: 84, batch:100, loss:3.6417696475982666\n",
      "epoch: 84, batch:200, loss:3.641751527786255\n",
      "epoch: 84, batch:300, loss:3.641735553741455\n",
      "epoch: 84, batch:400, loss:3.6417593955993652\n",
      "epoch: 85, batch:100, loss:3.6417577266693115\n",
      "epoch: 85, batch:200, loss:3.641768455505371\n",
      "epoch: 85, batch:300, loss:3.6417412757873535\n",
      "epoch: 85, batch:400, loss:3.6417441368103027\n",
      "epoch: 86, batch:100, loss:3.641765594482422\n",
      "epoch: 86, batch:200, loss:3.6417453289031982\n",
      "epoch: 86, batch:300, loss:3.641770362854004\n",
      "epoch: 86, batch:400, loss:3.641758680343628\n",
      "epoch: 87, batch:100, loss:3.6417603492736816\n",
      "epoch: 87, batch:200, loss:3.6417555809020996\n",
      "epoch: 87, batch:300, loss:3.641751527786255\n",
      "epoch: 87, batch:400, loss:3.641749620437622\n",
      "epoch: 88, batch:100, loss:3.6417500972747803\n",
      "epoch: 88, batch:200, loss:3.6417593955993652\n",
      "epoch: 88, batch:300, loss:3.6417579650878906\n",
      "epoch: 88, batch:400, loss:3.641758441925049\n",
      "epoch: 89, batch:100, loss:3.641749382019043\n",
      "epoch: 89, batch:200, loss:3.6417551040649414\n",
      "epoch: 89, batch:300, loss:3.641770839691162\n",
      "epoch: 89, batch:400, loss:3.6417434215545654\n",
      "epoch: 90, batch:100, loss:3.6417555809020996\n",
      "epoch: 90, batch:200, loss:3.64174222946167\n",
      "epoch: 90, batch:300, loss:3.641751527786255\n",
      "epoch: 90, batch:400, loss:3.6417481899261475\n",
      "epoch: 91, batch:100, loss:3.6417551040649414\n",
      "epoch: 91, batch:200, loss:3.641744613647461\n",
      "epoch: 91, batch:300, loss:3.6417438983917236\n",
      "epoch: 91, batch:400, loss:3.6417429447174072\n",
      "epoch: 92, batch:100, loss:3.6417436599731445\n",
      "epoch: 92, batch:200, loss:3.6417648792266846\n",
      "epoch: 92, batch:300, loss:3.641740083694458\n",
      "epoch: 92, batch:400, loss:3.6417319774627686\n",
      "epoch: 93, batch:100, loss:3.641745090484619\n",
      "epoch: 93, batch:200, loss:3.6417524814605713\n",
      "epoch: 93, batch:300, loss:3.641744613647461\n",
      "epoch: 93, batch:400, loss:3.6417508125305176\n",
      "epoch: 94, batch:100, loss:3.6417505741119385\n",
      "epoch: 94, batch:200, loss:3.641746997833252\n",
      "epoch: 94, batch:300, loss:3.6417429447174072\n",
      "epoch: 94, batch:400, loss:3.641730308532715\n",
      "epoch: 95, batch:100, loss:3.641749143600464\n",
      "epoch: 95, batch:200, loss:3.6417415142059326\n",
      "epoch: 95, batch:300, loss:3.6417553424835205\n",
      "epoch: 95, batch:400, loss:3.641742467880249\n",
      "epoch: 96, batch:100, loss:3.64174747467041\n",
      "epoch: 96, batch:200, loss:3.6417441368103027\n",
      "epoch: 96, batch:300, loss:3.641732692718506\n",
      "epoch: 96, batch:400, loss:3.6417505741119385\n",
      "epoch: 97, batch:100, loss:3.6417417526245117\n",
      "epoch: 97, batch:200, loss:3.6417391300201416\n",
      "epoch: 97, batch:300, loss:3.641742467880249\n",
      "epoch: 97, batch:400, loss:3.6417529582977295\n",
      "epoch: 98, batch:100, loss:3.6417481899261475\n",
      "epoch: 98, batch:200, loss:3.641758441925049\n",
      "epoch: 98, batch:300, loss:3.6417293548583984\n",
      "epoch: 98, batch:400, loss:3.641738176345825\n",
      "epoch: 99, batch:100, loss:3.6417291164398193\n",
      "epoch: 99, batch:200, loss:3.641749143600464\n",
      "epoch: 99, batch:300, loss:3.6417434215545654\n",
      "epoch: 99, batch:400, loss:3.641742467880249\n",
      "epoch: 100, batch:100, loss:3.6417343616485596\n",
      "epoch: 100, batch:200, loss:3.6417396068573\n",
      "epoch: 100, batch:300, loss:3.6417465209960938\n",
      "epoch: 100, batch:400, loss:3.641737937927246\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(100):\n",
    "    for batch_idx, (X,y) in enumerate(train_loader):\n",
    "        opt.zero_grad()\n",
    "        # print(X.shape)\n",
    "        # print(y.shape)\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        y_pred = model(X)\n",
    "        ls = loss(y_pred, y)\n",
    "        ls.backward()\n",
    "        opt.step()\n",
    "        if (batch_idx+1)%100 == 0 :\n",
    "            print('epoch: {}, batch:{}, loss:{}'.format(\n",
    "                epoch+1, batch_idx+1, ls\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1eb3b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YichuanMa\\AppData\\Local\\Temp\\ipykernel_29692\\4151872844.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "C:\\Users\\YichuanMa\\AppData\\Local\\Temp\\ipykernel_29692\\4151872844.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "model.eval()\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)  #每行中最大的数作为预测类别\n",
    "    cmp = y_hat.type(y.dtype) == y #转换数据类型后作比较\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "metric = [0., 0.]\n",
    "for batch_idx, (X,y) in enumerate(test_loader):\n",
    "    # print(X.shape)\n",
    "    # print(y.shape)\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    y_hat = model(X)\n",
    "    metric[0] += accuracy(y_hat, y)\n",
    "    metric[1] += len(y)\n",
    "\n",
    "print(metric[0] / metric[1])\n",
    "# 表现不是很理想 QAQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('CV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddddb6c8074598c347164dec535ee895b1608d6fc55cb3352f6ac3b714a65c93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
