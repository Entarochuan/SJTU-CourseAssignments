{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95c2060",
   "metadata": {},
   "source": [
    "#### Part1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9144, 200, 200, 3)\n",
      "(9144,)\n",
      "6400 train samples\n",
      "2744 test samples\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from keras import backend as K\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "IMAGE_SIZE = 200\n",
    "\n",
    "\n",
    "# 按照指定图像大小调整尺寸\n",
    "def resize_image(image, height=IMAGE_SIZE, width=IMAGE_SIZE):\n",
    "    return cv2.resize(image, (height, width))\n",
    "\n",
    "\n",
    "def read_path(path_name):\n",
    "    for dir_item in os.listdir(path_name):\n",
    "        full_path = os.path.abspath(os.path.join(path_name, dir_item))\n",
    "\n",
    "        if os.path.isdir(full_path):  # 如果是文件夹，继续递归调用\n",
    "            read_path(full_path)\n",
    "        else:  # 文件\n",
    "            if dir_item.endswith('.jpg') or dir_item.endswith('.JPG') or dir_item.endswith('.png'):\n",
    "                image = cv2.imread(full_path)\n",
    "                image = resize_image(image)\n",
    "                images.append(image)\n",
    "                labels.append(path_name)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_dataset(path_name):\n",
    "    images, labels = read_path(path_name)\n",
    "\n",
    "    images = np.array(images)\n",
    "    print(images.shape)\n",
    "    category = []\n",
    "    for i in labels:\n",
    "        category.append(i.split('/')[-1])\n",
    "    temp = list(set(category))\n",
    "    dic = {}\n",
    "    for i in range(len(temp)):\n",
    "        dic[temp[i]] = i\n",
    "    for i in range(len(category)):\n",
    "        labels[i] = dic[category[i]]\n",
    "    labels = np.array(labels)\n",
    "    print(labels.shape)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, path_name):\n",
    "        # 训练集\n",
    "        self.train_images = None\n",
    "        self.train_lb = None\n",
    "\n",
    "        # 测试集\n",
    "        self.test_images = None\n",
    "        self.test_lb = None\n",
    "\n",
    "        # 数据集加载路径\n",
    "        self.path_name = path_name\n",
    "\n",
    "        # 当前库采用的维度顺序\n",
    "        self.input_shape = None\n",
    "\n",
    "    # 加载数据集并按照交叉验证的原则划分数据集并进行相关预处理工作\n",
    "    def load(self, img_rows=IMAGE_SIZE, img_cols=IMAGE_SIZE,\n",
    "             img_channels=3, nb_classes=102):\n",
    "        # 加载数据集到内存\n",
    "        images, labels = load_dataset(self.path_name)\n",
    "\n",
    "        train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.3,\n",
    "                                                                                random_state=random.randint(0, 100))\n",
    "\n",
    "        # 当前的维度顺序如果为'th'，则输入图片数据时的顺序为：channels,rows,cols，否则:rows,cols,channels\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            train_images = train_images.reshape(train_images.shape[0], img_channels, img_rows, img_cols)\n",
    "            test_images = test_images.reshape(test_images.shape[0], img_channels, img_rows, img_cols)\n",
    "            self.input_shape = (img_channels, img_rows, img_cols)\n",
    "        else:\n",
    "            train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            self.input_shape = (img_rows, img_cols, img_channels)\n",
    "\n",
    "            # 输出训练集、验证集、测试集的数量\n",
    "            print(train_images.shape[0], 'train samples')\n",
    "            print(test_images.shape[0], 'test samples')\n",
    "\n",
    "            self.train_lb = train_labels\n",
    "            self.test_lb = test_labels\n",
    "\n",
    "            # 像素数据浮点化以便归一化\n",
    "            train_images = train_images.astype('float32')\n",
    "            test_images = test_images.astype('float32')\n",
    "\n",
    "            # 将其归一化,图像的各像素值归一化到0~1区间\n",
    "            train_images /= 255\n",
    "            test_images /= 255\n",
    "\n",
    "            self.train_images = train_images\n",
    "            self.test_images = test_images\n",
    "\n",
    "\n",
    "data = Dataset('./kaggle/caltech-101/')\n",
    "\n",
    "data.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a02cb",
   "metadata": {},
   "source": [
    "#### Part2 TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554dd133",
   "metadata": {},
   "source": [
    "TODO 利用SIFT从训练图像中提取特征  \n",
    "\n",
    "如果有需要，你也可以在pass之外的地方填写相关代码，请自便，下同。\n",
    "\n",
    "vec_dict 第i项： i为类别，对应的字典为所有属于该类的sift特征点的信息。注意：kp与des一一对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169be5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dict = {i:{'kp':[], 'des':[]} for i in range(102)}\n",
    "# des 为128维度的描述向量\n",
    "# 102 是数据集的类别个数...QAQ\n",
    "sift = cv2.SIFT_create()\n",
    "# print(data.train_images.shape[0])  # 6400\n",
    "for i in range(data.train_images.shape[0]):\n",
    "    # if i==10:\n",
    "    #     break\n",
    "    tep = cv2.normalize(data.train_images[i], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    kp_vector, des_vector = sift.detectAndCompute(tep, None)\n",
    "    class_num = data.train_lb[i]\n",
    "    \n",
    "    kp_vector = list(kp_vector)\n",
    "    des_vector = list(des_vector)\n",
    "    vec_dict[class_num]['kp'].extend(kp_vector)\n",
    "    # print(len(vec_dict[class_num]['kp']))\n",
    "    vec_dict[class_num]['des'].extend(des_vector)\n",
    "    # print(len(vec_dict[class_num]['kp']))\n",
    "    \n",
    "    # print(len(des_vector[0]))\n",
    "\n",
    "# print(vec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f640f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< cv2.KeyPoint 000002B8C25822E0>\n",
      "[ 47.  78. 120.  70.  19.   8.   1.   0. 136.  88.  21.   8.   1.   4.\n",
      "  10.  17.  16.  58.  20.   1.   1.  19.  14.   6.  50.  50.   3.   0.\n",
      "   0.   1.   0.   8. 131.  29.   5.   9. 142.  70.   6.   1. 152.  39.\n",
      "   1.   1.   7.  24.  45. 112.  41.   9.   2.   3.  74. 152.  68.  43.\n",
      "  86.  64.   1.   2.  13.  17.   3.   4.  54.   7.   0.   2.  95.  13.\n",
      "   1.   0. 152.  21.   0.   2.  13.   5.   4.  17.  42.   3.   0.  17.\n",
      " 149.  40.   6.   8.  28.  34.   2.   7.  30.   5.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "print(vec_dict[0]['kp'][0])\n",
    "print(vec_dict[0]['des'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71f9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "bneck_value = float(\"inf\")\n",
    "for i in range(102):\n",
    "    if len(vec_dict[i]['kp']) < bneck_value:\n",
    "        bneck_value = len(vec_dict[i]['kp'])\n",
    "        # print(bneck_value)\n",
    "  \n",
    "for i in range(102):\n",
    "    kp_list = vec_dict[i]['kp'] = sorted((vec_dict[i]['kp']),\n",
    "                                         key=lambda x: x.response,\n",
    "                                         reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce6a9c",
   "metadata": {},
   "source": [
    "TODO 为每个类别选择同样多的特征点用于聚类。特征点个数bneck_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0721b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_list = vec_dict[0]['des'][0:bneck_value]\n",
    "# print(bneck_value) 3379\n",
    "for i in range(1, 102):\n",
    "    vec_list = vec_list + vec_dict[i]['des'][0:bneck_value]\n",
    "vec_list = np.float64(vec_list)\n",
    "# print(vec_list.shape) (344658, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9a1f14ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 3326, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i in range(1, 102):\n",
    "#     #####\n",
    "#     pass\n",
    "#     #####\n",
    "# vec_list = np.float64(vec_list)\n",
    "\n",
    "vec_list = []\n",
    "vec_list.append(np.array(vec_dict[0]['des'][0:bneck_value], dtype=np.float64))\n",
    "# print(len(vec_list))\n",
    "for i in range(1, 102):\n",
    "    vec_list.append(np.array(vec_dict[i]['des'][0:bneck_value], dtype=np.float64))  # 不知道对不对 0.0\n",
    "vec_list = np.float64(vec_list)\n",
    "print(vec_list.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ec41d",
   "metadata": {},
   "source": [
    "TODO 对提取出的特征点使用Kmeans聚类，设定合适的聚类中心个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a721de84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=50)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#####\n",
    "N_clusters = 50 # just 4 try\n",
    "kmeans = KMeans(n_clusters=N_clusters)\n",
    "kmeans.fit(vec_list)\n",
    "#####\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc84c43",
   "metadata": {},
   "source": [
    "TODO 利用直方图统计每张图像中的特征点所属聚类中心的个数，将直方图归一化后便得到图像的特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "43aa1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = data.train_images.shape[0]\n",
    "hist_vector = np.zeros((num_images, N_clusters))\n",
    "for i in range(num_images):\n",
    "    # if i == 2:\n",
    "    #     break\n",
    "    tep = cv2.normalize(data.train_images[i], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    kp_vector, des_vector = sift.detectAndCompute(tep, None)\n",
    "    des_vector = np.array(des_vector, dtype=np.float64)\n",
    "    label_test = kmeans.predict(des_vector)\n",
    "    # print(label_test)\n",
    "    for label in label_test:\n",
    "        hist_vector[i][label] = hist_vector[i][label] + 1\n",
    "    \n",
    "    # dst = np.zeros_like(hist_vector[i])\n",
    "    # print(hist_vector)\n",
    "\n",
    "    # print(hist_vector[i])\n",
    "    cv2.normalize(hist_vector[i], hist_vector[i], 0, 255, cv2.NORM_MINMAX)\n",
    "    # print(hist_vector[i])\n",
    "# print(hist_vector[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3dd5e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120.78947368  93.94736842  67.10526316 107.36842105  67.10526316\n",
      "  26.84210526  53.68421053  67.10526316  40.26315789  40.26315789\n",
      " 161.05263158  53.68421053  13.42105263  53.68421053  26.84210526\n",
      "   0.          67.10526316   0.           0.         120.78947368\n",
      "  53.68421053  40.26315789  40.26315789  40.26315789  80.52631579\n",
      "  26.84210526  13.42105263  13.42105263  40.26315789  67.10526316\n",
      "  53.68421053  53.68421053  80.52631579 255.          80.52631579\n",
      "  40.26315789 120.78947368  53.68421053  80.52631579  93.94736842\n",
      "  53.68421053  26.84210526  40.26315789  40.26315789  67.10526316\n",
      "  67.10526316 107.36842105  67.10526316  80.52631579  67.10526316]\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "# print(data.train_lb[100])\n",
    "print(hist_vector[19])\n",
    "print(data.train_lb[6399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c812206a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier\n",
    "from sklearn import svm\n",
    "classifier = svm.SVC(probability=True)\n",
    "classifier.fit(hist_vector, data.train_lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace4f08",
   "metadata": {},
   "source": [
    "TODO 构建测试集并计算模型准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ca074621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.3261661807580175\n"
     ]
    }
   ],
   "source": [
    "# 使用SVM构建分类器\n",
    "# 你可以自行构建分类器，也可以使用SVM\n",
    "# TODO 构建测试集并计算模型准确率\n",
    "num_test_images = data.test_images.shape[0]\n",
    "hist_test_vector = np.zeros((num_test_images, N_clusters))\n",
    "# print(num_test_images)\n",
    "for i in range(num_test_images):\n",
    "    # if i==100:\n",
    "    #     break\n",
    "    tep = cv2.normalize(data.test_images[i], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    kp_vector, des_vector = sift.detectAndCompute(tep, None)\n",
    "    des_vector = np.array(des_vector, dtype=np.float64)\n",
    "    \n",
    "    label_pred_test = kmeans.predict(des_vector)\n",
    "    # label_pred_test = classifier.predict(vec_test)\n",
    "    for label in label_pred_test:\n",
    "        hist_test_vector[i][label] = hist_test_vector[i][label] + 1\n",
    "        \n",
    "    cv2.normalize(hist_test_vector[i], hist_test_vector[i], 0, 255, cv2.NORM_MINMAX)\n",
    "    # print(hist_test_vector[i])\n",
    "    # print()\n",
    "\n",
    "\n",
    "acc = classifier.predict(hist_test_vector)-data.test_lb\n",
    "tep = len(acc[acc==0])\n",
    "# print(tep)\n",
    "# print(len(data.test_lb))\n",
    "print('accuracy', tep/len(data.test_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4f990733",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(hist_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a103de39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "20\n",
      "9\n",
      "52\n",
      "3\n",
      "17\n",
      "94\n",
      "97\n",
      "47\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "07e3dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[255.  17.  34.  17.  17.  34.   0.  17.  17.  34.  17.  17.  51.   0.\n",
      "  51. 170.   0.  51.   0.  51.  51.  17.  34.  17.   0.  17.   0.   0.\n",
      "  34. 153.  17.  17.  17. 136.   0.  17.   0.  51.  68.  34.   0.  17.\n",
      "  17.  17.  34.  34.  17.  17.  17.   0.]\n"
     ]
    }
   ],
   "source": [
    "print(hist_test_vector[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cvhws')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "21f942a83a50423fda1faf7be65dffac700694899b08d6df5bd80ec7f9ec5b38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
